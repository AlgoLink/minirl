{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a1668ee-1d4b-46ac-9664-e2dc428c4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minirl import StdScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4d5199-f3cf-484e-8016-47eb149c5002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 0.0, 'y': 0.0}\n",
      "{'x': -1.0, 'y': -1.0000000000000007}\n",
      "{'x': 1.3750407018858473, 'y': -0.3849807180710737}\n",
      "{'x': -0.4677347527130586, 'y': 1.5929647630034436}\n",
      "{'x': 1.7183077766184363, 'y': 0.27682417143110466}\n",
      "{'x': 0.4564142614619115, 'y': 0.7369777254021131}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "X = [{'x': random.uniform(8, 12), 'y': random.uniform(8, 12)} for _ in range(6)]\n",
    "\n",
    "scaler = StdScaler()\n",
    "for x in X:\n",
    "    print(scaler.learn_one(x).transform_one(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec67a59-3895-45c0-bd2f-2e6a1d20fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module minirl.preprocessing.scaler:\n",
      "\n",
      "class StandardScaler(minirl.preprocessing.base.MiniBatchTransformer)\n",
      " |  StandardScaler(with_std=True)\n",
      " |  \n",
      " |  Scales the data so that it has zero mean and unit variance.\n",
      " |  Under the hood, a running mean and a running variance are maintained. The scaling is slightly\n",
      " |  different than when scaling the data in batch because the exact means and variances are not\n",
      " |  known in advance. However, this doesn't have a detrimental impact on performance in the long\n",
      " |  run.\n",
      " |  This transformer supports mini-batches as well as single instances. In the mini-batch case, the\n",
      " |  number of columns and the ordering of the columns are allowed to change between subsequent\n",
      " |  calls. In other words, this transformer will keep working even if you add and/or remove\n",
      " |  features every time you call `learn_many` and `transform_many`.\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  with_std\n",
      " |      Whether or not each feature should be divided by its standard deviation.\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import random\n",
      " |  >>> from minirl import StdScaler\n",
      " |  >>> random.seed(42)\n",
      " |  >>> X = [{'x': random.uniform(8, 12), 'y': random.uniform(8, 12)} for _ in range(6)]\n",
      " |  >>> for x in X:\n",
      " |  ...     print(x)\n",
      " |  {'x': 10.557, 'y': 8.100}\n",
      " |  {'x': 9.100, 'y': 8.892}\n",
      " |  {'x': 10.945, 'y': 10.706}\n",
      " |  {'x': 11.568, 'y': 8.347}\n",
      " |  {'x': 9.687, 'y': 8.119}\n",
      " |  {'x': 8.874, 'y': 10.021}\n",
      " |  >>> scaler = StdScaler()\n",
      " |  >>> for x in X:\n",
      " |  ...     print(scaler.learn_one(x).transform_one(x))\n",
      " |  {'x': 0.0, 'y': 0.0}\n",
      " |  {'x': -0.999, 'y': 0.999}\n",
      " |  {'x': 0.937, 'y': 1.350}\n",
      " |  {'x': 1.129, 'y': -0.651}\n",
      " |  {'x': -0.776, 'y': -0.729}\n",
      " |  {'x': -1.274, 'y': 0.992}\n",
      " |  This transformer also supports mini-batch updates. You can call `learn_many` and provide a\n",
      " |  `pandas.DataFrame`:\n",
      " |  >>> import pandas as pd\n",
      " |  >>> X = pd.DataFrame.from_dict(X)\n",
      " |  >>> scaler = StdScaler()\n",
      " |  >>> scaler = scaler.learn_many(X[:3])\n",
      " |  >>> scaler = scaler.learn_many(X[3:])\n",
      " |  You can then call `transform_many` to scale a mini-batch of features:\n",
      " |  >>> scaler.transform_many(X)\n",
      " |      x         y\n",
      " |  0  0.444600 -0.933384\n",
      " |  1 -1.044259 -0.138809\n",
      " |  2  0.841106  1.679208\n",
      " |  3  1.477301 -0.685117\n",
      " |  4 -0.444084 -0.914195\n",
      " |  5 -1.274664  0.992296\n",
      " |  References\n",
      " |  ----------\n",
      " |  [^1]: [Welford's Method (and Friends)](https://www.embeddedrelated.com/showarticle/785.php)\n",
      " |  [^2]: [Batch updates for simple statistics](https://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      minirl.preprocessing.base.MiniBatchTransformer\n",
      " |      minirl.preprocessing.base.Transformer\n",
      " |      minirl.preprocessing.base.Estimator\n",
      " |      minirl.preprocessing.base.Base\n",
      " |      abc.ABC\n",
      " |      minirl.preprocessing.base.BaseTransformer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  learn_many(self, X: pandas.core.frame.DataFrame)\n",
      " |      Update with a mini-batch of features.\n",
      " |      Note that the update formulas for mean and variance are slightly different than in the\n",
      " |      single instance case, but they produce exactly the same result.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X\n",
      " |          A dataframe where each column is a feature.\n",
      " |  \n",
      " |  learn_one(self, x)\n",
      " |      Update with a set of features `x`.\n",
      " |      A lot of transformers don't actually have to do anything during the `learn_one` step\n",
      " |      because they are stateless. For this reason the default behavior of this function is to do\n",
      " |      nothing. Transformers that however do something during the `learn_one` can override this\n",
      " |      method.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x\n",
      " |          A dictionary of features.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  transform_many(self, X: pandas.core.frame.DataFrame)\n",
      " |      Scale a mini-batch of features.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X\n",
      " |          A dataframe where each column is a feature. An exception will be raised if any of\n",
      " |          the features has not been seen during a previous call to `learn_many`.\n",
      " |  \n",
      " |  transform_one(self, x)\n",
      " |      Transform a set of features `x`.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x\n",
      " |          A dictionary of features.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      The transformed values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from minirl.preprocessing.base.Estimator:\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Merge with another Transformer into a Pipeline.\n",
      " |  \n",
      " |  __ror__(self, other)\n",
      " |      Merge with another Transformer into a Pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from minirl.preprocessing.base.Base:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  clone(self, new_params: dict = None, include_attributes=False)\n",
      " |      Return a fresh estimator with the same parameters.\n",
      " |      The clone has the same parameters but has not been updated with any data.\n",
      " |      This works by looking at the parameters from the class signature. Each parameter is either\n",
      " |      - recursively cloned if its a class.\n",
      " |      - deep-copied via `copy.deepcopy` if not.\n",
      " |      If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been\n",
      " |      seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to\n",
      " |      return a new instance with the same input parameters.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_params\n",
      " |      include_attributes\n",
      " |          Whether attributes that are not present in the class' signature should also be cloned\n",
      " |          or not.\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from river import linear_model\n",
      " |      >>> from river import optim\n",
      " |      >>> model = linear_model.LinearRegression(\n",
      " |      ...     optimizer=optim.SGD(lr=0.042),\n",
      " |      ... )\n",
      " |      >>> new_params = {\n",
      " |      ...     'optimizer': optim.SGD(.001)\n",
      " |      ... }\n",
      " |      >>> model.clone(new_params)\n",
      " |      LinearRegression (\n",
      " |        optimizer=SGD (\n",
      " |          lr=Constant (\n",
      " |            learning_rate=0.001\n",
      " |          )\n",
      " |        )\n",
      " |        loss=Squared ()\n",
      " |        l2=0.\n",
      " |        l1=0.\n",
      " |        intercept_init=0.\n",
      " |        intercept_lr=Constant (\n",
      " |          learning_rate=0.01\n",
      " |        )\n",
      " |        clip_gradient=1e+12\n",
      " |        initializer=Zeros ()\n",
      " |      )\n",
      " |      The algorithm is recursively called down `Pipeline`s and `TransformerUnion`s.\n",
      " |      >>> from river import compose\n",
      " |      >>> from river import preprocessing\n",
      " |      >>> model = compose.Pipeline(\n",
      " |      ...     preprocessing.StandardScaler(),\n",
      " |      ...     linear_model.LinearRegression(\n",
      " |      ...         optimizer=optim.SGD(0.042),\n",
      " |      ...     )\n",
      " |      ... )\n",
      " |      >>> new_params = {\n",
      " |      ...     'LinearRegression': {\n",
      " |      ...         'optimizer': optim.SGD(0.03)\n",
      " |      ...     }\n",
      " |      ... }\n",
      " |      >>> model.clone(new_params)\n",
      " |      Pipeline (\n",
      " |        StandardScaler (\n",
      " |          with_std=True\n",
      " |        ),\n",
      " |        LinearRegression (\n",
      " |          optimizer=SGD (\n",
      " |            lr=Constant (\n",
      " |              learning_rate=0.03\n",
      " |            )\n",
      " |          )\n",
      " |          loss=Squared ()\n",
      " |          l2=0.\n",
      " |          l1=0.\n",
      " |          intercept_init=0.\n",
      " |          intercept_lr=Constant (\n",
      " |            learning_rate=0.01\n",
      " |          )\n",
      " |          clip_gradient=1e+12\n",
      " |          initializer=Zeros ()\n",
      " |        )\n",
      " |      )\n",
      " |  \n",
      " |  mutate(self, new_attrs: dict)\n",
      " |      Modify attributes.\n",
      " |      This changes parameters inplace. Although you can change attributes yourself, this is the\n",
      " |      recommended way to proceed. By default, all attributes are immutable, meaning they\n",
      " |      shouldn't be mutated. Calling `mutate` on an immutable attribute raises a `ValueError`.\n",
      " |      Mutable attributes are specified via the `_mutable_attributes` property, and are thus\n",
      " |      specified on a per-estimator basis.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_attrs\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from river import linear_model\n",
      " |      >>> from river import optim\n",
      " |      >>> model = linear_model.LinearRegression(\n",
      " |      ...     optimizer=optim.SGD(0.042),\n",
      " |      ... )\n",
      " |      >>> new_params = {\n",
      " |      ...     'optimizer': {'lr': optim.schedulers.Constant(0.001)}\n",
      " |      ... }\n",
      " |      >>> model.mutate(new_params)\n",
      " |      >>> model\n",
      " |      LinearRegression (\n",
      " |        optimizer=SGD (\n",
      " |          lr=Constant (\n",
      " |            learning_rate=0.001\n",
      " |          )\n",
      " |        )\n",
      " |        loss=Squared ()\n",
      " |        l2=0.\n",
      " |        l1=0.\n",
      " |        intercept_init=0.\n",
      " |        intercept_lr=Constant (\n",
      " |          learning_rate=0.01\n",
      " |        )\n",
      " |        clip_gradient=1e+12\n",
      " |        initializer=Zeros ()\n",
      " |      )\n",
      " |      The algorithm is recursively called down `Pipeline`s and `TransformerUnion`s.\n",
      " |      >>> from river import compose\n",
      " |      >>> from river import preprocessing\n",
      " |      >>> model = compose.Pipeline(\n",
      " |      ...     preprocessing.StandardScaler(),\n",
      " |      ...     linear_model.LinearRegression(\n",
      " |      ...         optimizer=optim.SGD(lr=0.042),\n",
      " |      ...     )\n",
      " |      ... )\n",
      " |      >>> new_params = {\n",
      " |      ...     'LinearRegression': {\n",
      " |      ...         'l2': 5,\n",
      " |      ...         'optimizer': {'lr': optim.schedulers.Constant(0.03)}\n",
      " |      ...     }\n",
      " |      ... }\n",
      " |      >>> model.mutate(new_params)\n",
      " |      >>> model\n",
      " |      Pipeline (\n",
      " |        StandardScaler (\n",
      " |          with_std=True\n",
      " |        ),\n",
      " |        LinearRegression (\n",
      " |          optimizer=SGD (\n",
      " |            lr=Constant (\n",
      " |              learning_rate=0.03\n",
      " |            )\n",
      " |          )\n",
      " |          loss=Squared ()\n",
      " |          l2=5\n",
      " |          l1=0.\n",
      " |          intercept_init=0.\n",
      " |          intercept_lr=Constant (\n",
      " |            learning_rate=0.01\n",
      " |          )\n",
      " |          clip_gradient=1e+12\n",
      " |          initializer=Zeros ()\n",
      " |        )\n",
      " |      )\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from minirl.preprocessing.base.Base:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from minirl.preprocessing.base.BaseTransformer:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Fuses with another Transformer into a TransformerUnion.\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |  \n",
      " |  __radd__(self, other)\n",
      " |      Fuses with another Transformer into a TransformerUnion.\n",
      " |  \n",
      " |  __rmul__(self, other)\n",
      " |      Creates a Grouper.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StdScaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
