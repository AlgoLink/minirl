{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
    "        self.b2 = np.zeros((1, output_dim))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h = np.maximum(0, np.dot(X, self.W1) + self.b1)\n",
    "        out = np.dot(h, self.W2) + self.b2\n",
    "        return out\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        delta_output = output - y\n",
    "        delta_hidden = np.dot(delta_output, self.W2.T) * (self.W1 > 0)\n",
    "        h = np.maximum(0, np.dot(X, self.W1) + self.b1)\n",
    "        self.W2 -= 0.1 * np.dot(h.T, delta_output)\n",
    "        self.b2 -= 0.1 * np.sum(delta_output, axis=0, keepdims=True)\n",
    "        self.W1 -= 0.1 * np.dot(X.T, delta_hidden)\n",
    "        self.b1 -= 0.1 * np.sum(delta_hidden, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "def choose_action(model, context, epsilon):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return np.random.choice(model.output_dim)\n",
    "    else:\n",
    "        return np.argmax(model.forward(context))\n",
    "\n",
    "import random\n",
    "\n",
    "# generate some fake data\n",
    "contexts = np.random.randn(100, 1)\n",
    "actions = np.random.randint(3, size=100)\n",
    "rewards = np.zeros(100)\n",
    "for i in range(100):\n",
    "    if actions[i] == 0:\n",
    "        rewards[i] = np.random.normal(1, 0.1) * contexts[i]\n",
    "    elif actions[i] == 1:\n",
    "        rewards[i] = np.random.normal(2, 0.1) * contexts[i]\n",
    "    else:\n",
    "        rewards[i] = np.random.normal(0, 0.1) * contexts[i]\n",
    "\n",
    "# train the model\n",
    "model = Model(input_dim=1, output_dim=3, hidden_dim=10)\n",
    "epsilon = 0.1\n",
    "for t in range(1000):\n",
    "    idx = random.randint(0, len(contexts) - 1)\n",
    "    context = contexts[idx]\n",
    "    action = choose_action(model, context, epsilon)\n",
    "    reward = rewards[idx]\n",
    "    y = np.zeros(3)\n",
    "    y[action] = reward\n",
    "    output = model.forward(context.reshape(1,-1))\n",
    "    model.backward(context.reshape(1,-1), y, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.49031908e+00, -4.44896190e-01, -1.14228501e-02,\n",
       "        -1.40589308e+00, -9.64780214e-01, -1.51423217e-01,\n",
       "        -3.90400883e-01, -3.83275882e-04, -4.76242759e-01,\n",
       "        -7.79588045e-03]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50267041])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.09431802, 0.        ]),\n",
       " array([[-0.28618451, -0.03554534, -2.82803425]]),\n",
       " array([[-1.55637991]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,output,context.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28618451, -0.12986336, -2.82803425]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91272596, 0.21799937, 0.09747264]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(contexts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11498612,  0.0421402 , -1.13627631]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx=context.reshape(1,-1)\n",
    "model.forward(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11498612,  0.0421402 , -1.13627631]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-28 14:51:00 [info     ] APIs of mlopskit               model_name=feature_store model_version=1 ops_type=cache\n",
      "2023-05-28 14:51:01 [info     ] The list of all versions for the current model is \u001b[42;1m[1, 2]\u001b[0m.\n",
      "2023-05-28 14:51:01 [warning  ] The version 1 is out of date. You should consider upgrading to version `v2`.\n",
      "2023-05-28 14:51:01 [info     ] Usage of mlopskit-cache        Params=\u001b[36;1m{'db_type': 'rlite/redis/sfdb/diskcache, default:rlite', 'return_type': 'dblink/dbobj, default: dbobj', 'db_name': 'default: rlite_model.cache'}\u001b[0m\n",
      "An actor network is created.\n"
     ]
    }
   ],
   "source": [
    "from minirl.brain.deep_cb import DeepCBAgent\n",
    "from mlopskit import make\n",
    "\n",
    "model_db = make(\"cache/feature_store-v1\", db_name=\"deepcb.db\")\n",
    "\n",
    "test=DeepCBAgent(4,3,64,0.1,model_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    xxxx= test.act([1,0,0,0],\"test1\")\n",
    "    print(xxxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1) and (4,64) not aligned: 1 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test\u001b[39m.\u001b[39;49mlearn([\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m],\u001b[39m2\u001b[39;49m,\u001b[39m1.0\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mtest1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/minirl/brain/deep_cb.py:115\u001b[0m, in \u001b[0;36mDeepCBAgent.learn\u001b[0;34m(self, context, action, reward, model_id)\u001b[0m\n\u001b[1;32m    112\u001b[0m y[action] \u001b[39m=\u001b[39m reward\n\u001b[1;32m    113\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(context\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 115\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mbackward(context\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), y, output)\n\u001b[1;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_weights(model_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/minirl/brain/deep_cb.py:39\u001b[0m, in \u001b[0;36mModel.backward\u001b[0;34m(self, X, y, output)\u001b[0m\n\u001b[1;32m     37\u001b[0m W2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mdot(h\u001b[39m.\u001b[39mT, delta_output)\n\u001b[1;32m     38\u001b[0m b2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(delta_output, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 39\u001b[0m W1 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mdot(X\u001b[39m.\u001b[39;49mT, delta_hidden)\n\u001b[1;32m     40\u001b[0m b1 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(delta_hidden, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m W1\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,1) and (4,64) not aligned: 1 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "test.learn([1,0,0,0],2,1.0,\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,0,0,0]).reshape(1, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minirl.brain.deep_cb import DeepCB\n",
    "test=DeepCB(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    p=test.select_action([1,0,0,0])\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m a\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m      4\u001b[0m r\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m test\u001b[39m.\u001b[39;49mupdate(x,a,r)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/minirl/brain/deep_cb.py:150\u001b[0m, in \u001b[0;36mDeepCB.update\u001b[0;34m(self, x, a, r)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, x, a, r):\n\u001b[1;32m    148\u001b[0m     \u001b[39m# compute gradient of loss w.r.t. weights\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x)\n\u001b[0;32m--> 150\u001b[0m     probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(logits) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39;49msum(np\u001b[39m.\u001b[39;49mexp(logits), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, keepdims\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    151\u001b[0m     probs_for_actions \u001b[39m=\u001b[39m probs[\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(a)), a]\n\u001b[1;32m    152\u001b[0m     dlog \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2296\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2293\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   2294\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 2296\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   2297\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([1,0,0,0])\n",
    "a=0\n",
    "r=1.0\n",
    "test.update(x,a,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1000,2) (1000,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m]]) \u001b[39m# true coefficient matrix\u001b[39;00m\n\u001b[1;32m      9\u001b[0m noise \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(N, K) \u001b[39m# iid Gaussian noise\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X, theta) \u001b[39m+\u001b[39;49m noise \u001b[39m# matrix of rewards\u001b[39;00m\n\u001b[1;32m     12\u001b[0m cb \u001b[39m=\u001b[39m DeepCB(feature_dim\u001b[39m=\u001b[39mD, action_dim\u001b[39m=\u001b[39mK, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, num_hidden_layers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, hidden_layer_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N):\n\u001b[1;32m     15\u001b[0m     \u001b[39m# select action based on current context\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1000,2) (1000,3) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate random data for a simple 3-armed bandit problem\n",
    "N = 1000 # number of trials\n",
    "D = 2 # number of context features\n",
    "K = 3 # number of actions\n",
    "X = np.random.randn(N, D) # matrix of input contexts\n",
    "theta = np.array([[1, 0], [0, 2]]) # true coefficient matrix\n",
    "noise = np.random.randn(N, K) # iid Gaussian noise\n",
    "Y = np.dot(X, theta) + noise # matrix of rewards\n",
    "\n",
    "cb = DeepCB(feature_dim=D, action_dim=K, learning_rate=0.01, num_hidden_layers=2, hidden_layer_size=32)\n",
    "\n",
    "for t in range(N):\n",
    "    # select action based on current context\n",
    "    a = cb.select_action(X[t])\n",
    "    \n",
    "    # compute loss and update weights based on observed reward\n",
    "    prob = np.exp(cb.predict(X[t])) / np.sum(np.exp(cb.predict(X[t])))\n",
    "    loss = -np.log(prob[a])\n",
    "    cb.update(X[t], a, Y[t][a])\n",
    "\n",
    "# generate test data\n",
    "N_test = 100\n",
    "X_test = np.random.randn(N_test, D)\n",
    "Y_test = np.dot(X_test, theta) + np.random.randn(N_test, K)\n",
    "\n",
    "# evaluate performance of learned policy on test set\n",
    "regret = 0\n",
    "for t in range(N_test):\n",
    "    true_rewards = np.dot(X_test[t], theta)\n",
    "    optimal_action = np.argmax(true_rewards)\n",
    "    chosen_action = cb.select_action(X_test[t])\n",
    "    regret += true_rewards[optimal_action] - true_rewards[chosen_action]\n",
    "print('Average regret on test set: {:.2f}'.format(regret / N_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m     prob \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(cb\u001b[39m.\u001b[39mpredict(X[t])) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(cb\u001b[39m.\u001b[39mpredict(X[t])))\n\u001b[1;32m    101\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(prob[a])\n\u001b[0;32m--> 102\u001b[0m     cb\u001b[39m.\u001b[39;49mupdate(X[t], a, Y[t])\n\u001b[1;32m    104\u001b[0m \u001b[39m# evaluate performance of learned policy on test set\u001b[39;00m\n\u001b[1;32m    105\u001b[0m N_test \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 51\u001b[0m, in \u001b[0;36mDeepCB.update\u001b[0;34m(self, x, a, r)\u001b[0m\n\u001b[1;32m     49\u001b[0m logits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(logits)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_softmax(logits)\u001b[39m#np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m probs_for_actions \u001b[39m=\u001b[39m probs[\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(a)), a]\n\u001b[1;32m     52\u001b[0m dlog \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     53\u001b[0m dlog[\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(a)), a] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate random data for a simple 3-armed bandit problem\n",
    "N = 1000 # number of trials\n",
    "D = 2 # number of context features\n",
    "K = 3 # number of actions\n",
    "X = np.random.randn(N, D) # matrix of input contexts\n",
    "theta = np.array([[1, 0], [0, 2]]) # true coefficient matrix\n",
    "noise = np.random.randn(N) # iid Gaussian noise\n",
    "Y = np.dot(X, theta)[:, 0:1] + noise.reshape(-1, 1) # matrix of rewards\n",
    "\n",
    "# define the DeepCB class\n",
    "class DeepCB():\n",
    "    def __init__(self, feature_dim, action_dim, learning_rate=0.01, num_hidden_layers=1, hidden_layer_size=64):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        # initialize weight matrices for all layers\n",
    "        self.weights = {}\n",
    "        self.weights['W1'] = np.random.randn(feature_dim, hidden_layer_size) / np.sqrt(feature_dim)\n",
    "        self.weights['b1'] = np.zeros(hidden_layer_size)\n",
    "        for i in range(2, num_hidden_layers + 2):\n",
    "            self.weights['W{}'.format(i)] = np.random.randn(hidden_layer_size, hidden_layer_size) / np.sqrt(hidden_layer_size)\n",
    "            self.weights['b{}'.format(i)] = np.zeros(hidden_layer_size)\n",
    "        self.weights['WO'] = np.random.randn(hidden_layer_size, action_dim) / np.sqrt(hidden_layer_size)\n",
    "        self.weights['bO'] = np.zeros(action_dim)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # forward pass through network\n",
    "        h = x\n",
    "        for i in range(1, self.num_hidden_layers + 2):\n",
    "            h = np.maximum(0, np.dot(h, self.weights['W{}'.format(i)]) + self.weights['b{}'.format(i)])\n",
    "        output = np.dot(h, self.weights['WO']) + self.weights['bO']\n",
    "        return output\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "        Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "        log_probs = shifted_logits - np.log(Z)\n",
    "        probs = np.exp(log_probs)\n",
    "        return probs\n",
    "\n",
    "    def update(self, x, a, r):\n",
    "        # compute gradient of loss w.r.t. weights\n",
    "        logits = self.predict(x)\n",
    "        logits = np.array(logits).reshape(1,-1)\n",
    "        probs = self._softmax(logits)#np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "        probs_for_actions = probs[range(len(a)), a]\n",
    "        dlog = probs.copy()\n",
    "        dlog[range(len(a)), a] -= 1\n",
    "        dlog = dlog / len(r)\n",
    "        grads = {}\n",
    "        grads['WO'] = np.dot(self.weights['h'].T, dlog)\n",
    "        grads['bO'] = np.sum(dlog, axis=0)\n",
    "        dh = np.dot(dlog, self.weights['WO'].T)\n",
    "        dh[self.weights['h'] <= 0] = 0\n",
    "        for i in reversed(range(1, self.num_hidden_layers + 1)):\n",
    "            grads['W{}'.format(i)] = np.dot(self.weights['H{}'.format(i - 1)].T, dh)\n",
    "            grads['b{}'.format(i)] = np.sum(dh, axis=0)\n",
    "            dh = np.dot(dh, self.weights['W{}'.format(i)].T)\n",
    "            dh[self.weights['H{}'.format(i - 1)] <= 0] = 0\n",
    "        grads['W1'] = np.dot(x.T, dh)\n",
    "        grads['b1'] = np.sum(dh, axis=0)\n",
    "\n",
    "        # update weights using Adam optimizer\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-8\n",
    "        for param in self.weights:\n",
    "            m = np.zeros_like(self.weights[param])\n",
    "            v = np.zeros_like(self.weights[param])\n",
    "            t = 0\n",
    "            for i in range(grads[param].shape[0]):\n",
    "                g = grads[param][i, :]\n",
    "                t += 1\n",
    "                m = beta1 * m + (1 - beta1) * g\n",
    "                v = beta2 * v + (1 - beta2) * (g ** 2)\n",
    "                m_hat = m / (1 - beta1 ** t)\n",
    "                v_hat = v / (1 - beta2 ** t)\n",
    "                self.weights[param][i, :] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    def select_action(self, x):\n",
    "        # choose action with highest expected reward based on current context\n",
    "        logits = self.predict(x)\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return np.argmax(probs)\n",
    "\n",
    "# create an instance of the DeepCB class with appropriate hyperparameters\n",
    "cb = DeepCB(feature_dim=D, action_dim=K, learning_rate=0.01, num_hidden_layers=2, hidden_layer_size=32)\n",
    "\n",
    "# run a loop over the entire dataset, selecting an action for each context and updating the network weights based on the observed reward\n",
    "for t in range(N):\n",
    "    # select action based on current context\n",
    "    a = cb.select_action(X[t])\n",
    "    \n",
    "    # compute loss and update weights based on observed reward\n",
    "    prob = np.exp(cb.predict(X[t])) / np.sum(np.exp(cb.predict(X[t])))\n",
    "    loss = -np.log(prob[a])\n",
    "    cb.update(X[t], a, Y[t])\n",
    "\n",
    "# evaluate performance of learned policy on test set\n",
    "N_test = 100\n",
    "X_test = np.random.randn(N_test, D)\n",
    "true_rewards = np.dot(X_test, theta)[:, 0]\n",
    "regret = 0\n",
    "for t in range(N_test):\n",
    "    optimal_action = np.argmax(true_rewards[t])\n",
    "    chosen_action = cb.select_action(X_test[t])\n",
    "    regret += true_rewards[t][optimal_action] - true_rewards[t][chosen_action]\n",
    "print('Average regret on test set: {:.2f}'.format(regret / N_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(n_actions, p\u001b[39m=\u001b[39mprobs)\n\u001b[1;32m     66\u001b[0m     reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(reward_means[action], reward_vars[action])\n\u001b[0;32m---> 67\u001b[0m     bandit\u001b[39m.\u001b[39;49mupdate(context, action, reward, lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m     69\u001b[0m \u001b[39m# Evaluate the performance of the learned policy\u001b[39;00m\n\u001b[1;32m     70\u001b[0m n_trials \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n",
      "Cell \u001b[0;32mIn[16], line 34\u001b[0m, in \u001b[0;36mDeepContextualBandit.update\u001b[0;34m(self, context, action, reward, lr)\u001b[0m\n\u001b[1;32m     32\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(probs[action]) \u001b[39m*\u001b[39m reward\n\u001b[1;32m     33\u001b[0m dLdh2 \u001b[39m=\u001b[39m probs\n\u001b[0;32m---> 34\u001b[0m dLdh2[\u001b[39m0\u001b[39m, action] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     35\u001b[0m dLdW2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(np\u001b[39m.\u001b[39mtranspose(a1), dLdh2)\n\u001b[1;32m     36\u001b[0m dLdb2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dLdh2, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepContextualBandit:\n",
    "    \n",
    "    def __init__(self, n_actions, n_features, hidden_layers):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Initialize the network weights and biases\n",
    "        self.W1 = np.random.randn(self.n_features, self.hidden_layers[0])\n",
    "        self.b1 = np.zeros((1, self.hidden_layers[0]))\n",
    "        \n",
    "        self.W2 = np.random.randn(self.hidden_layers[0], self.n_actions)\n",
    "        self.b2 = np.zeros((1, self.n_actions))\n",
    "        \n",
    "        self.weights = [self.W1, self.b1, self.W2, self.b2]\n",
    "        \n",
    "    def policy(self, context):\n",
    "        # Compute the output probabilities for each possible action\n",
    "        h1 = np.dot(context, self.W1) + self.b1\n",
    "        a1 = np.tanh(h1)\n",
    "        h2 = np.dot(a1, self.W2) + self.b2\n",
    "        exp_scores = np.exp(h2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        return probs.squeeze()\n",
    "    \n",
    "    def update(self, context, action, reward, lr):\n",
    "        # Compute the loss and gradients\n",
    "        probs = self.policy(context)\n",
    "        loss = -np.log(probs[action]) * reward\n",
    "        dLdh2 = probs\n",
    "        dLdh2[0, action] -= 1\n",
    "        dLdW2 = np.dot(np.transpose(a1), dLdh2)\n",
    "        dLdb2 = np.sum(dLdh2, axis=0, keepdims=True)\n",
    "        da1dh1 = 1 - np.square(a1)\n",
    "        dLda1 = np.dot(dLdh2, np.transpose(self.W2))\n",
    "        dLdh1 = dLda1 * da1dh1\n",
    "        dLdW1 = np.dot(np.transpose(context), dLdh1)\n",
    "        dLdb1 = np.sum(dLdh1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update the weights\n",
    "        self.W2 -= lr * dLdW2\n",
    "        self.b2 -= lr * dLdb2\n",
    "        self.W1 -= lr * dLdW1\n",
    "        self.b1 -= lr * dLdb1\n",
    "        self.weights = [self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the reward distribution\n",
    "reward_means = np.array([0.1, 0.5, 0.9])\n",
    "reward_vars = np.array([0.01, 0.05, 0.1])\n",
    "n_actions = len(reward_means)\n",
    "n_features = 10\n",
    "hidden_layers = [20, 10]\n",
    "\n",
    "# Initialize the contextual bandit and generate training data\n",
    "bandit = DeepContextualBandit(n_actions, n_features, hidden_layers)\n",
    "n_episodes = 1000\n",
    "for episode in range(n_episodes):\n",
    "    context = np.random.randn(1, n_features)\n",
    "    probs = bandit.policy(context)\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "    reward = np.random.normal(reward_means[action], reward_vars[action])\n",
    "    bandit.update(context, action, reward, lr=0.01)\n",
    "\n",
    "# Evaluate the performance of the learned policy\n",
    "n_trials = 1000\n",
    "total_reward = 0\n",
    "for trial in range(n_trials):\n",
    "    context = np.random.randn(1, n_features)\n",
    "    probs = bandit.policy(context)\n",
    "    action = np.argmax(probs)\n",
    "    reward = np.random.normal(reward_means[action], reward_vars[action])\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Average reward over {} trials: {}\".format(n_trials, total_reward / n_trials))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(n_actions, p\u001b[39m=\u001b[39mprobs)\n\u001b[1;32m     79\u001b[0m     reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(reward_means[action], reward_vars[action])\n\u001b[0;32m---> 80\u001b[0m     bandit\u001b[39m.\u001b[39;49mupdate(context, action, reward, lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m \u001b[39m# Evaluate the performance of the learned policy\u001b[39;00m\n\u001b[1;32m     83\u001b[0m n_trials \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 42\u001b[0m, in \u001b[0;36mDeepContextualBandit.update\u001b[0;34m(self, context, action, reward, lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m dLdh2 \u001b[39m=\u001b[39m probs\n\u001b[1;32m     41\u001b[0m dLdh2[action] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 42\u001b[0m dLdW2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(np\u001b[39m.\u001b[39mtranspose(a1), dLdh2)\n\u001b[1;32m     43\u001b[0m dLdb2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dLdh2, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m da1dh1 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39msquare(a1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a1' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepContextualBandit:\n",
    "    \n",
    "    def __init__(self, n_actions, n_features, hidden_layers):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Initialize the network weights and biases\n",
    "        self.W1 = np.random.randn(self.n_features, self.hidden_layers[0])\n",
    "        self.b1 = np.zeros((1, self.hidden_layers[0]))\n",
    "        \n",
    "        self.W2 = np.random.randn(self.hidden_layers[0], self.n_actions)\n",
    "        self.b2 = np.zeros((1, self.n_actions))\n",
    "        \n",
    "        self.weights = [self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "        # Initialize Adam optimizer variables\n",
    "        self.m = [np.zeros_like(w) for w in self.weights]\n",
    "        self.v = [np.zeros_like(w) for w in self.weights]\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "    def policy(self, context):\n",
    "        # Compute the output probabilities for each possible action\n",
    "        h1 = np.dot(context, self.W1) + self.b1\n",
    "        a1 = np.tanh(h1)\n",
    "        h2 = np.dot(a1, self.W2) + self.b2\n",
    "        exp_scores = np.exp(h2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        return probs.squeeze()\n",
    "    \n",
    "    def update(self, context, action, reward, lr):\n",
    "        # Compute the loss and gradients\n",
    "        probs = self.policy(context)\n",
    "        loss = -np.log(probs[action]) * reward\n",
    "        dLdh2 = probs\n",
    "        dLdh2[action] -= 1\n",
    "        dLdW2 = np.dot(np.transpose(a1), dLdh2)\n",
    "        dLdb2 = np.sum(dLdh2, axis=0, keepdims=True)\n",
    "        da1dh1 = 1 - np.square(a1)\n",
    "        dLda1 = np.dot(dLdh2, np.transpose(self.W2))\n",
    "        dLdh1 = dLda1 * da1dh1\n",
    "        dLdW1 = np.dot(np.transpose(context), dLdh1)\n",
    "        dLdb1 = np.sum(dLdh1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update the Adam optimizer variables\n",
    "        self.m = [self.beta1 * m + (1 - self.beta1) * g for m, g in zip(self.m, [dLdW1, dLdb1, dLdW2, dLdb2])]\n",
    "        self.v = [self.beta2 * v + (1 - self.beta2) * np.square(g) for v, g in zip(self.v, [dLdW1, dLdb1, dLdW2, dLdb2])]\n",
    "        m_hat = [m / (1 - self.beta1) for m in self.m]\n",
    "        v_hat = [v / (1 - self.beta2) for v in self.v]\n",
    "        \n",
    "        # Update the weights using the Adam optimizer\n",
    "        self.W2 -= lr * m_hat[2] / (np.sqrt(v_hat[2]) + self.epsilon)\n",
    "        self.b2 -= lr * m_hat[3] / (np.sqrt(v_hat[3]) + self.epsilon)\n",
    "        self.W1 -= lr * m_hat[0] / (np.sqrt(v_hat[0]) + self.epsilon)\n",
    "        self.b1 -= lr * m_hat[1] / (np.sqrt(v_hat[1]) + self.epsilon)\n",
    "        self.weights = [self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "\n",
    "\n",
    "# Define the reward distribution\n",
    "reward_means = np.array([0.1, 0.5, 0.9])\n",
    "reward_vars = np.array([0.01, 0.05, 0.1])\n",
    "n_actions = len(reward_means)\n",
    "n_features = 10\n",
    "hidden_layers = [20, 10]\n",
    "\n",
    "# Initialize the contextual bandit and generate training data\n",
    "bandit = DeepContextualBandit(n_actions, n_features, hidden_layers)\n",
    "n_episodes = 1000\n",
    "for episode in range(n_episodes):\n",
    "    context = np.random.randn(1, n_features)\n",
    "    probs = bandit.policy(context)\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "    reward = np.random.normal(reward_means[action], reward_vars[action])\n",
    "    bandit.update(context, action, reward, lr=0.01)\n",
    "\n",
    "# Evaluate the performance of the learned policy\n",
    "n_trials = 1000\n",
    "total_reward = 0\n",
    "for trial in range(n_trials):\n",
    "    context = np.random.randn(1, n_features)\n",
    "    probs = bandit.policy(context)\n",
    "    action = np.argmax(probs)\n",
    "    reward = np.random.normal(reward_means[action], reward_vars[action])\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Average reward over {} trials: {}\".format(n_trials, total_reward / n_trials))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44397411, 0.45982688, 0.09619901])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 4\n",
    "n_actions=3\n",
    "# Initialize the contextual bandit and generate training data\n",
    "bandit = DeepContextualBandit(n_actions, n_features, hidden_layers)\n",
    "\n",
    "bandit.policy([1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m action\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m      3\u001b[0m reward\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m bandit\u001b[39m.\u001b[39;49mupdate(context, action, reward, lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[24], line 42\u001b[0m, in \u001b[0;36mDeepContextualBandit.update\u001b[0;34m(self, context, action, reward, lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m dLdh2 \u001b[39m=\u001b[39m probs\n\u001b[1;32m     41\u001b[0m dLdh2[action] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 42\u001b[0m dLdW2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(np\u001b[39m.\u001b[39mtranspose(a1), dLdh2)\n\u001b[1;32m     43\u001b[0m dLdb2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dLdh2, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m da1dh1 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39msquare(a1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a1' is not defined"
     ]
    }
   ],
   "source": [
    "context = [1,0,0,0]\n",
    "action=0\n",
    "reward=1\n",
    "bandit.update(context, action, reward, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10) [0.         1.74178014 0.         0.         0.        ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (5,) and (64,10) not aligned: 5 (dim 0) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(env\u001b[39m.\u001b[39mnum_actions, p\u001b[39m=\u001b[39mprobs)\n\u001b[1;32m     81\u001b[0m reward \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mget_reward(action, context)\n\u001b[0;32m---> 82\u001b[0m grads \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mbackward(context, probs, action, reward)\n\u001b[1;32m     84\u001b[0m \u001b[39m# Update weights using Adam optimizer\u001b[39;00m\n\u001b[1;32m     85\u001b[0m beta1 \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m\n",
      "Cell \u001b[0;32mIn[41], line 54\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[0;34m(self, x, y, action, reward)\u001b[0m\n\u001b[1;32m     52\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i)]\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(W\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mshape,dL_dy)\n\u001b[0;32m---> 54\u001b[0m dh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(dL_dy, W\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m     55\u001b[0m dW \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(h\u001b[39m.\u001b[39mT, (dh \u001b[39m*\u001b[39m (h \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)))\n\u001b[1;32m     56\u001b[0m db \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dh \u001b[39m*\u001b[39m (h \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,) and (64,10) not aligned: 5 (dim 0) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "class Environment:\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def generate_context(self):\n",
    "        return np.random.normal(size=(10,))\n",
    "    \n",
    "    def get_reward(self, action, context):\n",
    "        return np.random.normal(loc=action + np.dot(context, np.random.normal(size=(10,))), scale=0.5)\n",
    "\n",
    "# Define the neural network\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        self.params = {}\n",
    "        \n",
    "        # Initialize weights for each layer\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.params['W' + str(i)] = np.random.randn(dims[i], dims[i+1])\n",
    "            self.params['b' + str(i)] = np.zeros(dims[i+1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for i in range(len(self.params) // 2 - 1):\n",
    "            W = self.params['W' + str(i)]\n",
    "            b = self.params['b' + str(i)]\n",
    "            h = np.maximum(0, np.dot(h, W) + b)\n",
    "        W = self.params['W' + str(len(self.params) // 2 - 1)]\n",
    "        b = self.params['b' + str(len(self.params) // 2 - 1)]\n",
    "        y = softmax(np.dot(h, W) + b)\n",
    "        return y\n",
    "        \n",
    "    def backward(self, x, y, action, reward):\n",
    "        grads = {}\n",
    "        \n",
    "        # Calculate derivative of loss w.r.t. output\n",
    "        dL_dy = np.zeros(y.shape)\n",
    "        dL_dy[action] = -reward / y[action]\n",
    "        \n",
    "        # Backpropagate through network\n",
    "        h = x\n",
    "\n",
    "        for i in range(len(self.params) // 2 - 1):\n",
    "            W = self.params['W' + str(i)]\n",
    "            b = self.params['b' + str(i)]\n",
    "            print(W.T.shape,dL_dy)\n",
    "            dh = np.dot(dL_dy, W.T)\n",
    "            dW = np.dot(h.T, (dh * (h > 0)))\n",
    "            db = np.sum(dh * (h > 0), axis=0)\n",
    "            grads['W' + str(i)] = dW\n",
    "            grads['b' + str(i)] = db\n",
    "            dL_dy = dh * (h > 0)\n",
    "            h = np.maximum(0, np.dot(h, W.T) + b)\n",
    "        W = self.params['W' + str(len(self.params) // 2 - 1)]\n",
    "        b = self.params['b' + str(len(self.params) // 2 - 1)]\n",
    "        dW = np.dot(h.T, dL_dy)\n",
    "        db = np.sum(dL_dy, axis=0)\n",
    "        grads['W' + str(len(self.params) // 2 - 1)] = dW\n",
    "        grads['b' + str(len(self.params) // 2 - 1)] = db\n",
    "        \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "# Implement the training loop\n",
    "env = Environment(num_actions=5)\n",
    "nn = NeuralNetwork(input_dim=10, hidden_dims=[64, 32], output_dim=5)\n",
    "num_steps = 10000\n",
    "lr = 0.01\n",
    "for i in range(num_steps):\n",
    "    context = env.generate_context()\n",
    "    probs = nn.forward(context)\n",
    "    action = np.random.choice(env.num_actions, p=probs)\n",
    "    reward = env.get_reward(action, context)\n",
    "    grads = nn.backward(context, probs, action, reward)\n",
    "    \n",
    "    # Update weights using Adam optimizer\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps = 1e-8\n",
    "    t = i + 1\n",
    "    for param_name in nn.params:\n",
    "        m = np.zeros_like(nn.params[param_name])\n",
    "        v = np.zeros_like(nn.params[param_name])\n",
    "        m = beta1 * m + (1 - beta1) * grads[param_name]\n",
    "        v = beta2 * v + (1 - beta2) * (grads[param_name] ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        nn.params[param_name] -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "# Evaluate the performance\n",
    "cumulative_reward = 0\n",
    "optimal_reward = 0\n",
    "for i in range(num_steps):\n",
    "    context = env\n",
    "    probs = nn.forward(context)\n",
    "    action = np.argmax(probs)\n",
    "    reward = env.get_reward(action, context)\n",
    "    cumulative_reward += reward\n",
    "    optimal_reward += np.max([env.get_reward(a, context) for a in range(env.num_actions)])\n",
    "regret = optimal_reward - cumulative_reward\n",
    "print('Regret:', regret)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (50,5) (10,5) (50,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m r \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m#simulate_reward(x, a)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# Update policy based on observed reward signal\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m bandit\u001b[39m.\u001b[39;49mupdate(x, a, r)\n",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mDeepContextualBandit.update\u001b[0;34m(self, x, a, r, lr)\u001b[0m\n\u001b[1;32m     20\u001b[0m dloss_dtheta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dloss_dlogits\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mT) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mtanh(np\u001b[39m.\u001b[39mdot(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m dloss_dtheta\n\u001b[0;32m---> 22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m dloss_dW\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (50,5) (10,5) (50,5) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepContextualBandit:\n",
    "    def __init__(self, n_context_features, n_actions, n_hidden_units):\n",
    "        self.theta = np.random.normal(size=(n_context_features, n_hidden_units))\n",
    "        self.W = np.random.normal(size=(n_hidden_units, n_actions))\n",
    "\n",
    "    def predict(self, x):\n",
    "        hidden_layer = np.dot(x, self.theta)\n",
    "        logits = np.dot(hidden_layer, self.W)\n",
    "        return softmax(logits)\n",
    "\n",
    "    def update(self, x, a, r, lr=0.001):\n",
    "        p = self.predict(x)\n",
    "        log_prob = np.log(p[a])\n",
    "        loss = -log_prob * r\n",
    "        dloss_dlogits = p.copy()\n",
    "        dloss_dlogits[a] -= 1\n",
    "        dloss_dW = np.dot(x.T.reshape(-1,1), dloss_dlogits.reshape(1,-1))\n",
    "        dloss_dtheta = np.dot(dloss_dlogits.reshape(1,-1), self.W.T) * (1 - np.tanh(np.dot(x, self.theta))**2)\n",
    "        self.theta -= lr * dloss_dtheta\n",
    "        self.W -= lr * dloss_dW\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "# Example usage\n",
    "n_context_features = 10\n",
    "n_actions = 5\n",
    "n_hidden_units = 50\n",
    "bandit = DeepContextualBandit(n_context_features, n_actions, n_hidden_units)\n",
    "num_iterations=4\n",
    "epsilon=0.1\n",
    "for i in range(num_iterations):\n",
    "    # Sample context\n",
    "    x = np.random.normal(size=n_context_features)\n",
    "\n",
    "    # Make decision based on current policy\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.choice(np.arange(n_actions))\n",
    "    else:\n",
    "        a = np.argmax(bandit.predict(x))\n",
    "\n",
    "    # Sample reward\n",
    "    r = 1.0 #simulate_reward(x, a)\n",
    "\n",
    "    # Update policy based on observed reward signal\n",
    "    bandit.update(x, a, r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,5) (1,20) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m     x \u001b[39m=\u001b[39m X_train[i]\n\u001b[1;32m     53\u001b[0m     y \u001b[39m=\u001b[39m y_train[i]\n\u001b[0;32m---> 54\u001b[0m     p \u001b[39m=\u001b[39m gln\u001b[39m.\u001b[39;49mforward(x\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     55\u001b[0m     gln\u001b[39m.\u001b[39mbackward(x\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), y, p)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Generate some test data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 18\u001b[0m, in \u001b[0;36mGatedLinearNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m h \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39mdot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb1)  \u001b[39m# ReLU activation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m g \u001b[39m=\u001b[39m sigmoid(np\u001b[39m.\u001b[39mdot(h, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW2) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb2)  \u001b[39m# Sigmoid gating\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m g \u001b[39m*\u001b[39;49m h\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,5) (1,20) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class GatedLinearNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = np.maximum(0, np.dot(X, self.W1) + self.b1)  # ReLU activation\n",
    "        g = sigmoid(np.dot(h, self.W2) + self.b2)  # Sigmoid gating\n",
    "        return g * h  # Element-wise multiplication for gating\n",
    "\n",
    "    def backward(self, X, y, p, lr=0.01):\n",
    "        # Compute gradients using backpropagation\n",
    "        dh = (p - y) @ self.W2.T * (self.forward(X) > 0)\n",
    "        dW2 = self.forward(X).T @ (p - y)\n",
    "        db2 = np.sum(p - y, axis=0)\n",
    "        dW1 = X.T @ dh\n",
    "        db1 = np.sum(dh, axis=0)\n",
    "\n",
    "        # Update weights and biases using stochastic gradient descent\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the GLN architecture\n",
    "input_size = 10  # number of features in the input context\n",
    "hidden_size = 20  # number of hidden units in the GLN\n",
    "output_size = 5  # number of possible actions in the bandit problem\n",
    "gln = GatedLinearNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate some training data\n",
    "X_train = np.random.randn(1000, input_size)\n",
    "y_train = np.random.randint(output_size, size=1000)\n",
    "\n",
    "# Train the GLN using online learning\n",
    "for i in range(len(X_train)):\n",
    "    x = X_train[i]\n",
    "    y = y_train[i]\n",
    "    p = gln.forward(x.reshape(1, -1))\n",
    "    gln.backward(x.reshape(1, -1), y, p)\n",
    "\n",
    "# Generate some test data\n",
    "X_test = np.random.randn(100, input_size)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gln.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the predictions\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h_relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msquare(q_target \u001b[39m-\u001b[39m q_values)\u001b[39m.\u001b[39msum()\n\u001b[1;32m     64\u001b[0m grad_q_values \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (q_values \u001b[39m-\u001b[39m q_target)\n\u001b[0;32m---> 66\u001b[0m grad_W2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mouter(h_relu, grad_q_values)\n\u001b[1;32m     67\u001b[0m grad_b2 \u001b[39m=\u001b[39m grad_q_values\n\u001b[1;32m     68\u001b[0m grad_h_relu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(grad_q_values, W2\u001b[39m.\u001b[39mT)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h_relu' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.random.randn(hidden_size)\n",
    "\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.random.randn(output_size)\n",
    "\n",
    "# Define the forward pass function\n",
    "def forward(x):\n",
    "    h = np.matmul(x, W1) + b1\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = np.matmul(h_relu, W2) + b2\n",
    "    return y_pred\n",
    "\n",
    "# Define the reward function\n",
    "def reward(state, action):\n",
    "    # Return a random reward for this example\n",
    "    return np.random.normal()\n",
    "\n",
    "# Define the update function using the Adam optimizer\n",
    "def update(params, grad, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    m = params.get(\"m\", 0)\n",
    "    v = params.get(\"v\", 0)\n",
    "    t = params.get(\"t\", 0)\n",
    "\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * np.square(grad)\n",
    "    t += 1\n",
    "\n",
    "    m_hat = m / (1 - beta1 ** t)\n",
    "    v_hat = v / (1 - beta2 ** t)\n",
    "\n",
    "    params[\"m\"] = m\n",
    "    params[\"v\"] = v\n",
    "    params[\"t\"] = t\n",
    "    \n",
    "    params -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "# Train the agent\n",
    "n_episodes = 1000\n",
    "n_steps_per_episode = 100\n",
    "\n",
    "params = {\"m\": 0, \"v\": 0, \"t\": 0}\n",
    "for i in range(n_episodes):\n",
    "    state = np.random.randn(input_size)\n",
    "    for j in range(n_steps_per_episode):\n",
    "        action_values = forward(state)\n",
    "        action = np.argmax(action_values)\n",
    "\n",
    "        r = reward(state, action)\n",
    "\n",
    "        # Compute the loss and gradients\n",
    "        q_values = forward(state)\n",
    "        q_target = q_values.copy()\n",
    "        q_target[action] = r + np.max(q_values)\n",
    "\n",
    "        loss = np.square(q_target - q_values).sum()\n",
    "        grad_q_values = 2 * (q_values - q_target)\n",
    "\n",
    "        grad_W2 = np.outer(h_relu, grad_q_values)\n",
    "        grad_b2 = grad_q_values\n",
    "        grad_h_relu = np.matmul(grad_q_values, W2.T)\n",
    "        grad_h = grad_h_relu.copy()\n",
    "        grad_h[h < 0] = 0\n",
    "        grad_W1 = np.outer(x, grad_h)\n",
    "        grad_b1 = grad_h\n",
    "\n",
    "        # Update the weights using the Adam optimizer\n",
    "        grads = {\"W1\": grad_W1, \"b1\": grad_b1, \"W2\": grad_W2, \"b2\": grad_b2}\n",
    "        for param_name, grad in grads.items():\n",
    "            update(params[param_name], grad)\n",
    "        \n",
    "        state = np.random.randn(input_size)\n",
    "\n",
    "# Evaluate the agent\n",
    "n_test_episodes = 100\n",
    "total_reward = 0\n",
    "for i in range(n_test_episodes):\n",
    "    state = np.random.randn(input_size)\n",
    "    for j in range(n_steps_per_episode):\n",
    "        action_values = forward(state)\n",
    "        action = np.argmax(action_values)\n",
    "\n",
    "        r = reward(state, action)\n",
    "        total_reward += r\n",
    "\n",
    "        state = np.random.randn(input_size)\n",
    "\n",
    "avg_reward = total_reward / (n_test_episodes * n_steps_per_episode)\n",
    "print(\"Average reward:\", avg_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_reward(features, action):\n",
    "    # Generate a reward for the chosen action based on the true weights and feature values\n",
    "    return np.dot(features, true_weights[:, action])\n",
    "\n",
    "class DeepContextualBandit:\n",
    "    def __init__(self, n_features, n_actions, learning_rate=0.01):\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize the model weights randomly\n",
    "        self.weights = np.random.randn(n_features, n_actions)\n",
    "        \n",
    "        # Initialize the Adam optimizer parameters\n",
    "        self.m = np.zeros_like(self.weights)\n",
    "        self.v = np.zeros_like(self.weights)\n",
    "        self.t = 0\n",
    "        \n",
    "    def predict(self, features):\n",
    "        # Predict the action probabilities for the given features\n",
    "        logits = np.dot(features, self.weights)\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return probs\n",
    "    \n",
    "    def update(self, features, action, reward):\n",
    "        # Compute the TD-error for the chosen action\n",
    "        probs = self.predict(features)\n",
    "        td_error = reward - np.dot(features, self.weights[:, action])\n",
    "        \n",
    "        # Compute the gradients with respect to the weights\n",
    "        grad = np.outer(features, -td_error * probs)\n",
    "        \n",
    "        # Update the Adam optimizer parameters\n",
    "        self.t += 1\n",
    "        self.m = 0.9 * self.m + 0.1 * grad\n",
    "        self.v = 0.999 * self.v + 0.001 * grad**2\n",
    "        \n",
    "        # Compute the bias-corrected estimates of the first and second moments\n",
    "        m_hat = self.m / (1 - 0.9**self.t)\n",
    "        v_hat = self.v / (1 - 0.999**self.t)\n",
    "        \n",
    "        # Update the model weights using the Adam optimizer\n",
    "        eps = 1e-8\n",
    "        alpha = self.learning_rate * np.sqrt(1 - 0.999**self.t) / (1 - 0.9**self.t)\n",
    "        self.weights -= alpha * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "# Define the number of features and actions\n",
    "n_features = 5\n",
    "n_actions = 3\n",
    "\n",
    "# Initialize the true weights randomly\n",
    "true_weights = np.random.randn(n_features, n_actions)\n",
    "\n",
    "# Create an instance of the DeepContextualBandit class\n",
    "model = DeepContextualBandit(n_features, n_actions)\n",
    "\n",
    "# Generate a set of features for testing\n",
    "features = np.random.randn(n_features)\n",
    "\n",
    "# Choose an action based on the predicted probabilities\n",
    "probs = model.predict(features)\n",
    "action = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "# Generate a reward for the chosen action based on the true weights and feature values\n",
    "reward = get_reward(features, action)\n",
    "\n",
    "# Update the model weights using the Adam optimizer and TD-error loss\n",
    "model.update(features, action, reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    probs = model.predict(features)\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = 10\n",
    "_action = 0\n",
    "for i in range(190):\n",
    "    model.update(features, _action, reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0538958 , -0.45592182, -0.11156368,  2.26930846,  1.83955083])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.08419051, -0.01487196, -0.08447032],\n",
       "       [-0.04604203,  0.72478887, -1.55600925],\n",
       "       [-0.56125922, -0.2741829 , -0.07345757],\n",
       "       [-0.11980817, -0.21165303,  0.64059228],\n",
       "       [ 0.28152997,  0.0765191 ,  0.46429352]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 172\u001b[0m\n\u001b[1;32m    170\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(context, [\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    171\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(action)\n\u001b[0;32m--> 172\u001b[0m     model\u001b[39m.\u001b[39;49mupdate(x, y)\n\u001b[1;32m    174\u001b[0m \u001b[39m# Test the model on 1000 new contexts\u001b[39;00m\n\u001b[1;32m    175\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[88], line 64\u001b[0m, in \u001b[0;36mDeepContextualBandit.update\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     61\u001b[0m softmax_out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(z2) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(z2), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m dsoftmax \u001b[39m=\u001b[39m softmax_out\n\u001b[0;32m---> 64\u001b[0m dsoftmax[y] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     66\u001b[0m dW2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dsoftmax, a1\u001b[39m.\u001b[39mT)\n\u001b[1;32m     67\u001b[0m db2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39msum(dsoftmax, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size, \u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepContextualBandit:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        ob_n = input_size\n",
    "        H = hidden_size\n",
    "        ac_n=output_size\n",
    "        self.W1 = (-1 + 2 * np.random.rand(ob_n, H)) / np.sqrt(ob_n)\n",
    "        self.b1 = np.zeros(H)\n",
    "        self.W2= (-1 + 2 * np.random.rand(H, ac_n)) / np.sqrt(H)\n",
    "        self.b2 = np.zeros(ac_n)\n",
    "\n",
    "\n",
    "        # self.W1 = np.random.randn(hidden_size, input_size) / np.sqrt(input_size)\n",
    "        # self.b1 = np.zeros((hidden_size, 1))\n",
    "        # self.W2 = np.random.randn(output_size, hidden_size) / np.sqrt(hidden_size)\n",
    "        # self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Forward pass through the network\n",
    "        #z1 = np.dot(self.W1, x) + self.b1\n",
    "        #a1 = np.maximum(z1, 0)\n",
    "        #z2 = np.dot(self.W2, a1) + self.b2\n",
    "\n",
    "        # forward computations\n",
    "        W1,b1=self.W1,self.b1\n",
    "        W2,b2 = self.W2,self.b2\n",
    "    \n",
    "        affine1 = x.dot(W1) + b1\n",
    "        relu1 = np.maximum(0, affine1)\n",
    "        affine2 = relu1.dot(W2) + b2\n",
    "\n",
    "        logits = affine2  # layer right before softmax (i also call this h)\n",
    "        # pass through a softmax to get probabilities\n",
    "        probs = self._softmax(logits)\n",
    "\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "        Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "        log_probs = shifted_logits - np.log(Z)\n",
    "        probs = np.exp(log_probs)\n",
    "        return probs\n",
    "\n",
    "    def update(self, x, y):\n",
    "        \n",
    "        # Compute gradients using backpropagation\n",
    "        #z1 = np.dot(self.W1, x) + self.b1\n",
    "        z1 = x.dot(self.W1)+self.b1\n",
    "        a1 = np.maximum(z1, 0)\n",
    "        #z2 = np.dot(self.W2, a1) + self.b2\n",
    "        z2=a1.dot(self.W2)+self.b2\n",
    "        softmax_out = np.exp(z2) / np.sum(np.exp(z2), axis=0)\n",
    "\n",
    "        dsoftmax = softmax_out\n",
    "        dsoftmax[y] -= 1\n",
    "\n",
    "        dW2 = np.dot(dsoftmax, a1.T)\n",
    "        db2 = np.reshape(np.sum(dsoftmax, axis=1), (self.output_size, 1))\n",
    "\n",
    "        da1 = np.dot(self.W2.T, dsoftmax)\n",
    "        dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "        dW1 = np.dot(dz1, x.T)\n",
    "        db1 = np.reshape(np.sum(dz1, axis=1), (self.hidden_size, 1))\n",
    "\n",
    "        # Update weights using Adam optimizer\n",
    "        self.W1, self.b1, self.W2, self.b2 = adam(self.W1, self.b1, self.W2, self.b2, dW1, db1, dW2, db2, self.learning_rate)\n",
    "\n",
    "def adam(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Initialize moments to zero\n",
    "    W1_m = np.zeros_like(W1)\n",
    "    b1_m = np.zeros_like(b1)\n",
    "    W2_m = np.zeros_like(W2)\n",
    "    b2_m = np.zeros_like(b2)\n",
    "\n",
    "    # Initialize variances to zero\n",
    "    W1_v = np.zeros_like(W1)\n",
    "    b1_v = np.zeros_like(b1)\n",
    "    W2_v = np.zeros_like(W2)\n",
    "    b2_v = np.zeros_like(b2)\n",
    "\n",
    "    t = 0 # initialize timestep\n",
    "\n",
    "    # Update biased moment estimates\n",
    "    W1_m = beta1 * W1_m + (1 - beta1) * dW1\n",
    "    b1_m = beta1 * b1_m + (1 - beta1) * db1\n",
    "    W2_m = beta1 * W2_m + (1 - beta1) * dW2\n",
    "    b2_m = beta1 * b2_m + (1 - beta1) * db2\n",
    "\n",
    "    # Update biased variance estimates\n",
    "    W1_v = beta2 * W1_v + (1 - beta2) * np.square(dW1)\n",
    "    b1_v = beta2 * b1_v + (1 - beta2) * np.square(db1)\n",
    "    W2_v = beta2 * W2_v + (1 - beta2) * np.square(dW2)\n",
    "    b2_v = beta2 * b2_v + (1 - beta2) * np.square(db2)\n",
    "\n",
    "    # Compute bias-corrected moment and variance estimates\n",
    "    t += 1\n",
    "    W1_m_corr = W1_m / (1 - beta1 ** t)\n",
    "    b1_m_corr = b1_m / (1 - beta1 ** t)\n",
    "    W2_m_corr = W2_m / (1 - beta1 ** t)\n",
    "    b2_m_corr = b2_m / (1 - beta1 ** t)\n",
    "\n",
    "    W1_v_corr = W1_v / (1 - beta2 ** t)\n",
    "    b1_v_corr = b1_v / (1 - beta2 ** t)\n",
    "    W2_v_corr = W2_v / (1 - beta2 ** t)\n",
    "    b2_v_corr = b2_v / (1 - beta2 ** t)\n",
    "\n",
    "    # Update weights\n",
    "    W1 -= learning_rate * W1_m_corr / (np.sqrt(W1_v_corr) + eps)\n",
    "    b1 -= learning_rate * b1_m_corr / (np.sqrt(b1_v_corr) + eps)\n",
    "    W2 -= learning_rate * W2_m_corr / (np.sqrt(W2_v_corr) + eps)\n",
    "    b2 -= learning_rate * b2_m_corr / (np.sqrt(b2_v_corr) + eps)\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, num_actions, context_dim):\n",
    "        self.num_actions = num_actions\n",
    "        self.context_dim = context_dim\n",
    "        self.theta = np.random.randn(context_dim, num_actions)\n",
    "\n",
    "    def get_context(self):\n",
    "        # Generate a random context vector\n",
    "        return np.random.randn(self.context_dim)\n",
    "\n",
    "    def get_reward(self, context, action):\n",
    "        # Compute the reward for the given context and action\n",
    "        return np.dot(context, self.theta[:,action]) + np.random.randn()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create the contextual bandit environment\n",
    "num_actions = 5\n",
    "context_dim = 10\n",
    "bandit = Bandit(num_actions, context_dim)\n",
    "\n",
    "# Create the deep contextual bandit model with hidden layer size 50\n",
    "model = DeepContextualBandit(context_dim, 50, num_actions, 0.001)\n",
    "\n",
    "# Train the model for 10000 steps\n",
    "for i in range(10000):\n",
    "    # Get a random context and choose an action based on the current policy\n",
    "    context = bandit.get_context()\n",
    "    #action = model.predict(np.reshape(context, (context_dim,1)))\n",
    "    obs = np.reshape(context,[1,-1])\n",
    "    action =model.predict(obs)\n",
    "    action = np.argmax(action)\n",
    "\n",
    "    # Take the chosen action and observe the reward\n",
    "    reward = bandit.get_reward(context, action)\n",
    "\n",
    "    # Update the model weights using Adam optimization\n",
    "    #x = np.reshape(context, (context_dim,1))\n",
    "    x = np.reshape(context, [1,-1])\n",
    "    y = np.array(action)\n",
    "    model.update(x, y)\n",
    "\n",
    "# Test the model on 1000 new contexts\n",
    "total_reward = 0.0\n",
    "for i in range(1000):\n",
    "    # Get a new context and choose an action based on the learned policy\n",
    "    context = bandit.get_context()\n",
    "    #action = model.predict(np.reshape(context, (context_dim,1)))\n",
    "    action = model.predict(context)\n",
    "    action = np.argmax(action)\n",
    "\n",
    "    # Take the chosen action and observe the reward\n",
    "    reward = bandit.get_reward(context, action)\n",
    "    total_reward += reward\n",
    "\n",
    "# Print the average reward across all test contexts\n",
    "print(\"Average reward:\", total_reward / 1000.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15354161],\n",
       "       [-0.68235045],\n",
       "       [-1.48879375],\n",
       "       [-0.55490513],\n",
       "       [-0.83321522],\n",
       "       [-1.24108735],\n",
       "       [ 1.93751965],\n",
       "       [-0.17739913],\n",
       "       [ 0.14284871],\n",
       "       [-0.51570859]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context\n",
    "np.reshape(context, (context_dim,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.10204149,  0.85236363,  0.98037945, -0.86044372,  0.05011679,\n",
       "         0.5403553 ,  0.75912338, -0.521934  ,  2.06914535,  0.3697579 ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs=context\n",
    "obs = np.reshape(obs, [1, -1])\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "\n",
    "\n",
    "class DeepContextualBandit():\n",
    "    def __init__(self, num_actions, num_features, hidden_size=64, learning_rate=0.01):\n",
    "        self.num_actions = num_actions\n",
    "        self.num_features = num_features\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Initialize weights for each layer\n",
    "        self.weights1 = np.random.normal(size=(num_features, hidden_size))\n",
    "        self.weights2 = np.random.normal(size=(hidden_size, num_actions))\n",
    "        \n",
    "        # Initialize biases for each layer\n",
    "        self.biases1 = np.zeros(hidden_size)\n",
    "        self.biases2 = np.zeros(num_actions)\n",
    "        \n",
    "        # Initialize optimizer: Adam\n",
    "        self.m1 = np.zeros_like(self.weights1)\n",
    "        self.v1 = np.zeros_like(self.weights1)\n",
    "        self.m2 = np.zeros_like(self.weights2)\n",
    "        self.v2 = np.zeros_like(self.weights2)\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.eps = 1e-8\n",
    "        self.epsilon = self.eps\n",
    "\n",
    "    def predict(self, context):\n",
    "        context = np.array(context).reshape(1,-1)\n",
    "        hidden = relu(np.dot(context, self.weights1) + self.biases1)\n",
    "        output = softmax(np.dot(hidden, self.weights2) + self.biases2)\n",
    "        action = choice(self.num_actions, p=output[0])\n",
    "        return action\n",
    "\n",
    "    def update2(self, context, action, reward):\n",
    "        context = np.array(context).reshape(1,-1)\n",
    "        \n",
    "        target = np.zeros(self.num_actions)\n",
    "        target[action] = 1.0\n",
    "        \n",
    "        # Forward pass\n",
    "        hidden = relu(np.dot(context, self.weights1) + self.biases1)\n",
    "        output = softmax(np.dot(hidden, self.weights2) + self.biases2)\n",
    "        \n",
    "        # Backward pass\n",
    "        d_output = output - target\n",
    "        d_hidden = np.dot(d_output, self.weights2.T) * (hidden > 0)\n",
    "        \n",
    "        # Update weights and biases using Adam optimizer\n",
    "        grad2 = np.dot(hidden.T, d_output)\n",
    "        grad1 = np.dot(context.T, d_hidden)\n",
    "        self.m1 = self.beta1 * self.m1 + (1 - self.beta1) * grad1\n",
    "        self.v1 = self.beta2 * self.v1 + (1 - self.beta2) * (grad1 ** 2)\n",
    "        self.m2 = self.beta1 * self.m2 + (1 - self.beta1) * grad2\n",
    "        self.v2 = self.beta2 * self.v2 + (1 - self.beta2) * (grad2 ** 2)\n",
    "        m_hat1 = self.m1 / (1 - self.beta1)\n",
    "        v_hat1 = self.v1 / (1 - self.beta2)\n",
    "        m_hat2 = self.m2 / (1 - self.beta1)\n",
    "        v_hat2 = self.v2 / (1 - self.beta2)\n",
    "        self.weights1 += self.learning_rate * m_hat1 / (np.sqrt(v_hat1) + self.eps)\n",
    "        self.weights2 += self.learning_rate * m_hat2 / (np.sqrt(v_hat2) + self.eps)\n",
    "        self.biases1 += self.learning_rate * np.mean(d_hidden, axis=0)\n",
    "        self.biases2 += self.learning_rate * np.mean(d_output, axis=0)\n",
    "\n",
    "    def update(self, context, action, reward):\n",
    "        context = np.array(context).reshape(1,-1)\n",
    "        # Forward pass\n",
    "        hidden = np.maximum(0, np.dot(context, self.weights1) + self.biases1)\n",
    "        output = np.dot(hidden, self.weights2) + self.biases2\n",
    "\n",
    "        # Compute loss and gradients\n",
    "        y = np.zeros_like(output)\n",
    "        y[action] = 1\n",
    "        loss = (reward - output[action])**2\n",
    "        d_output = -(y - output)\n",
    "        d_hidden = np.dot(d_output, self.weights2.T) * (hidden > 0)\n",
    "\n",
    "        # Update weights and biases using Adam optimizer\n",
    "        grad2 = np.dot(hidden.T, d_output)\n",
    "        grad1 = np.dot(context.T, d_hidden)\n",
    "        self.m1 = self.beta1 * self.m1 + (1 - self.beta1) * grad1\n",
    "        self.m2 = self.beta1 * self.m2 + (1 - self.beta1) * grad2\n",
    "        self.v1 = self.beta2 * self.v1 + (1 - self.beta2) * grad1**2\n",
    "        self.v2 = self.beta2 * self.v2 + (1 - self.beta2) * grad2**2\n",
    "        self.weights1 -= self.lr * self.m1 / (np.sqrt(self.v1) + self.epsilon)\n",
    "        self.biases1 -= self.lr * np.mean(d_hidden, axis=0)\n",
    "        self.weights2 -= self.lr * self.m2 / (np.sqrt(self.v2) + self.epsilon)\n",
    "        self.biases2 -= self.lr * np.mean(d_output, axis=0)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Initialize deep contextual bandit model\n",
    "\n",
    "num_features =4\n",
    "num_actions = 3\n",
    "model = DeepContextualBandit(num_actions, num_features, hidden_size=64, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    a=model.predict([1,0,0,0])\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeepContextualBandit' object has no attribute 'bias1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m context\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model\u001b[39m.\u001b[39;49mupdate(context,action,reward)\n",
      "Cell \u001b[0;32mIn[160], line 92\u001b[0m, in \u001b[0;36mDeepContextualBandit.update\u001b[0;34m(self, context, action, reward)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta2 \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv2 \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta2) \u001b[39m*\u001b[39m grad2\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights1 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm1 \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv1) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)\n\u001b[0;32m---> 92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias1 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(d_hidden, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm2 \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv2) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(d_output, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DeepContextualBandit' object has no attribute 'bias1'"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    reward=1.0\n",
    "    context=[1,0,0,0]\n",
    "    action = 0\n",
    "    model.update(context,action,reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f06f4baf162c1bba868b3ed890ca059fd0c13f705dc9d48413e36c65b86dd60d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
