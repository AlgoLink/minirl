{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepContextualBandit:\n",
    "    def __init__(self, num_actions, context_size, hidden_size, learning_rate):\n",
    "        self.num_actions = num_actions\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weights for the neural network\n",
    "        self.W1 = np.random.randn(context_size, hidden_size) / np.sqrt(context_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, num_actions) / np.sqrt(hidden_size)\n",
    "        self.b2 = np.zeros((1, num_actions))\n",
    "        \n",
    "        # initialize optimizer\n",
    "        self.optimizer = Adam(learning_rate)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # forward pass through the neural network\n",
    "        h1 = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        out = np.dot(h1, self.W2) + self.b2\n",
    "        return out\n",
    "    \n",
    "    def update(self, x, a, r):\n",
    "        # compute the gradient w.r.t. the action taken\n",
    "        grad = self.softmax_grad(self.predict(x), a)\n",
    "        \n",
    "        # compute the loss and update the weights\n",
    "        loss = -r * np.log(grad)\n",
    "        grad_wrt_out = grad.copy()\n",
    "        grad_wrt_out[0,a] -= 1\n",
    "        grad_wrt_h1 = np.dot(grad_wrt_out, self.W2.T)\n",
    "        h1 = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        grad_wrt_h1[h1 <= 0] = 0\n",
    "        grad_wrt_W2 = np.dot(h1.T, grad_wrt_out)\n",
    "        grad_wrt_b2 = np.sum(grad_wrt_out, axis=0, keepdims=True)\n",
    "        grad_wrt_W1 = np.dot(x.T, grad_wrt_h1)\n",
    "        grad_wrt_b1 = np.sum(grad_wrt_h1, axis=0, keepdims=True)\n",
    "        \n",
    "        # update the weights using Adam optimizer\n",
    "        self.W1, self.b1, self.W2, self.b2 = self.optimizer.update(\n",
    "            self.W1, self.b1, self.W2, self.b2,\n",
    "            grad_wrt_W1, grad_wrt_b1, grad_wrt_W2, grad_wrt_b2\n",
    "        )\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    \n",
    "    def softmax_grad(self, out, a):\n",
    "        grad = self.softmax(out)\n",
    "        grad[0,a] -= 1\n",
    "        return grad\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m_t = None\n",
    "        self.v_t = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, W1, b1, W2, b2, grad_wrt_W1, grad_wrt_b1, grad_wrt_W2, grad_wrt_b2):\n",
    "        self.t += 1\n",
    "        \n",
    "        if self.m_t is None:\n",
    "            self.m_t = [np.zeros_like(W1), np.zeros_like(b1), np.zeros_like(W2), np.zeros_like(b2)]\n",
    "            self.v_t = [np.zeros_like(W1), np.zeros_like(b1), np.zeros_like(W2), np.zeros_like(b2)]\n",
    "        \n",
    "        m_t_prev = self.m_t.copy()\n",
    "        v_t_prev = self.v_t.copy()\n",
    "        \n",
    "        self.m_t[0] = self.beta1 * m_t_prev[0] + (1 - self.beta1) * grad_wrt_W1\n",
    "        self.m_t[1] = self.beta1 * m_t_prev[1] + (1 - self.beta1) * grad_wrt_b1\n",
    "        self.m_t[2] = self.beta1 * m_t_prev[2] + (1 - self.beta1) * grad_wrt_W2\n",
    "        self.m_t[3] = self.beta1 * m_t_prev[3] + (1 - self.beta1) * grad_wrt_b2\n",
    "        \n",
    "        self.v_t[0] = self.beta2 * v_t_prev[0] + (1 - self.beta2) * (grad_wrt_W1 ** 2)\n",
    "        self.v_t[1] = self.beta2 * v_t_prev[1] + (1 - self.beta2) * (grad_wrt_b1 ** 2)\n",
    "        self.v_t[2] = self.beta2 * v_t_prev[2] + (1 - self.beta2) * (grad_wrt_W2 ** 2)\n",
    "        self.v_t[3] = self.beta2 * v_t_prev[3] + (1 - self.beta2) * (grad_wrt_b2 ** 2)\n",
    " \n",
    "        m_t_hat_0 = self.m_t[0] / (1 - self.beta1 ** self.t)\n",
    "        m_t_hat_1 = self.m_t[1] / (1 - self.beta1 ** self.t)\n",
    "        m_t_hat_2 = self.m_t[2] / (1 - self.beta1 ** self.t)\n",
    "        m_t_hat_3 = self.m_t[3] / (1 - self.beta1 ** self.t)\n",
    " \n",
    "        v_t_hat_0 = self.v_t[0] / (1 - self.beta2 ** self.t)\n",
    "        v_t_hat_1 = self.v_t[1] / (1 - self.beta2 ** self.t)\n",
    "        v_t_hat_2 = self.v_t[2] / (1 - self.beta2 ** self.t)\n",
    "        v_t_hat_3 = self.v_t[3] / (1 - self.beta2 ** self.t)\n",
    " \n",
    "        W1 -= self.learning_rate * m_t_hat_0 / (np.sqrt(v_t_hat_0) + self.epsilon)\n",
    "        b1 -= self.learning_rate * m_t_hat_1 / (np.sqrt(v_t_hat_1) + self.epsilon)\n",
    "        W2 -= self.learning_rate * m_t_hat_2 / (np.sqrt(v_t_hat_2) + self.epsilon)\n",
    "        b2 -= self.learning_rate * m_t_hat_3 / (np.sqrt(v_t_hat_3) + self.epsilon)\n",
    " \n",
    "        return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 4\n",
    "context_size = 10\n",
    "hidden_size = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "env = ContextualBandit(num_actions, context_size)\n",
    "agent = DeepContextualBandit(num_actions, context_size, hidden_size, learning_rate)\n",
    "\n",
    "for i in range(10000):\n",
    "# get a context from the environment\n",
    "context = env.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinUCB:\n",
    "    \"\"\"\n",
    "    LinUCB algorithm implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim,action_n,alpha, context=\"user\"):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : number\n",
    "            LinUCB parameter\n",
    "        context: string\n",
    "            'user' or 'both'(item+user): what to use as a feature vector\n",
    "        \"\"\"\n",
    "        self.n_features =obs_dim\n",
    "        if context == \"user\":\n",
    "            self.context = 1\n",
    "        elif context == \"both\":\n",
    "            self.context = 2\n",
    "            self.n_features *= 2\n",
    "\n",
    "        self.A = np.array([np.identity(self.n_features)] * action_n)\n",
    "        self.A_inv = np.array([np.identity(self.n_features)] * action_n)\n",
    "        self.b = np.zeros((action_n, self.n_features, 1))\n",
    "        self.alpha = round(alpha, 1)\n",
    "        self.algorithm = \"LinUCB (Î±=\" + str(self.alpha) + \", context:\" + context + \")\"\n",
    "\n",
    "    def choose_arm(self, context):\n",
    "        \"\"\"\n",
    "        Returns the best arm's index relative to the pool\n",
    "        Parameters\n",
    "        ----------\n",
    "        user : array\n",
    "            user features\n",
    "        pool_idx : array of indexes\n",
    "            pool indexes for article identification\n",
    "        \"\"\"\n",
    "\n",
    "        A_inv = self.A_inv[pool_idx]\n",
    "        b = self.b[pool_idx]\n",
    "\n",
    "        n_pool = len(pool_idx)\n",
    "\n",
    "        user = np.array([user] * n_pool)\n",
    "        if self.context == 1:\n",
    "            x = user\n",
    "        else:\n",
    "            x = np.hstack((user, dataset.features[pool_idx]))\n",
    "\n",
    "        x = x.reshape(n_pool, self.n_features, 1)\n",
    "\n",
    "        theta = A_inv @ b\n",
    "\n",
    "        p = np.transpose(theta, (0, 2, 1)) @ x + self.alpha * np.sqrt(\n",
    "            np.transpose(x, (0, 2, 1)) @ A_inv @ x\n",
    "        )\n",
    "        return np.argmax(p)\n",
    "\n",
    "    def update(self, context, action, reward,features=None):\n",
    "        \"\"\"\n",
    "        Updates algorithm's parameters(matrices) : A,b\n",
    "        Parameters\n",
    "        ----------\n",
    "        displayed : index\n",
    "            displayed article index relative to the pool\n",
    "        reward : binary\n",
    "            user clicked or not\n",
    "        user : array\n",
    "            user features\n",
    "        pool_idx : array of indexes\n",
    "            pool indexes for article identification\n",
    "        \"\"\"\n",
    "\n",
    "        a = action # displayed article's index\n",
    "        if self.context == 1:\n",
    "            x = np.array(context)\n",
    "        else:\n",
    "            x = np.hstack((context, features[a]))\n",
    "\n",
    "        x = x.reshape((self.n_features, 1))\n",
    "\n",
    "        self.A[a] += x @ x.T\n",
    "        self.b[a] += reward * x\n",
    "        self.A_inv[a] = np.linalg.inv(self.A[a])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Basic LinUCB implementation.\n",
    "'''\n",
    "\n",
    "# Python imports.\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Other imports.\n",
    "#from simple_rl.agents.AgentClass import Agent\n",
    "''' AgentClass.py: Class for a basic RL Agent '''\n",
    "\n",
    "# Python imports.\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent(object):\n",
    "    ''' Abstract Agent class. '''\n",
    "\n",
    "    def __init__(self, name, actions, gamma=0.99):\n",
    "        self.name = name\n",
    "        self.actions = list(actions) # Just in case we're given a numpy array (like from Atari).\n",
    "        self.gamma = gamma\n",
    "        self.episode_number = 0\n",
    "        self.prev_state = None\n",
    "        self.prev_action = None\n",
    "\n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            (dict) key=param_name (str) --> val=param_val (object).\n",
    "        '''\n",
    "        return {}\n",
    "\n",
    "    def act(self, state, reward):\n",
    "        '''\n",
    "        Args:\n",
    "            state (State): see StateClass.py\n",
    "            reward (float): the reward associated with arriving in state @state.\n",
    "\n",
    "        Returns:\n",
    "            (str): action.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def policy(self, state):\n",
    "        return self.act(state, 0)\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Summary:\n",
    "            Resets the agent back to its tabula rasa config.\n",
    "        '''\n",
    "        self.prev_state = None\n",
    "        self.prev_action = None\n",
    "        self.step_number = 0\n",
    "\n",
    "    def end_of_episode(self):\n",
    "        '''\n",
    "        Summary:\n",
    "            Resets the agents prior pointers.\n",
    "        '''\n",
    "        self.prev_state = None\n",
    "        self.prev_action = None\n",
    "        self.episode_number += 1\n",
    "\n",
    "    def set_name(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.name)\n",
    "\n",
    "\n",
    "class LinUCBAgent(Agent):\n",
    "    '''\n",
    "    From:\n",
    "        Lihong Li, et al. \"A Contextual-Bandit Approach to Personalized\n",
    "        News Article Recommendation.\" In Proceedings of the 19th\n",
    "        International Conference on World Wide Web (WWW), 2010.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, actions, name=\"LinUCB\", rand_init=True, context_size=1, alpha=1.5):\n",
    "        '''\n",
    "        Args:\n",
    "            actions (list): Contains a string for each action.\n",
    "            name (str)\n",
    "            context_size (int)\n",
    "            alpha (float): Uncertainty parameter.\n",
    "        '''\n",
    "        Agent.__init__(self, name, actions)\n",
    "        self.alpha = alpha\n",
    "        self.context_size = context_size\n",
    "        self.prev_context = None\n",
    "        self.step_number = 0\n",
    "        self.rand_init = rand_init\n",
    "        self._init_action_model(rand_init)\n",
    "\n",
    "\n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            (dict) key=param_name (str) --> val=param_val (object).\n",
    "        '''\n",
    "        param_dict = defaultdict(int)\n",
    "        \n",
    "        param_dict[\"rand_init\"] = self.rand_init\n",
    "        param_dict[\"context_size\"] = self.context_size\n",
    "        param_dict[\"alpha\"] = self.alpha\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def _init_action_model(self, rand_init=True):\n",
    "        '''\n",
    "        Summary:\n",
    "            Initializes model parameters\n",
    "        '''\n",
    "        self.model = {'act': {}, 'act_inv': {}, 'theta': {}, 'b': {}}\n",
    "        for action_id in range(len(self.actions)):\n",
    "            self.model['act'][action_id] = np.identity(self.context_size)\n",
    "            self.model['act_inv'][action_id] = np.identity(self.context_size)\n",
    "            if rand_init:\n",
    "                self.model['theta'][action_id] = np.random.random((self.context_size, 1))\n",
    "            else:\n",
    "                self.model['theta'][action_id] = np.zeros((self.context_size, 1))\n",
    "            self.model['b'][action_id] = np.zeros((self.context_size,1))\n",
    "\n",
    "    def _compute_score(self, context):\n",
    "        '''\n",
    "        Args:\n",
    "            context (list)\n",
    "\n",
    "        Returns:\n",
    "            (dict):\n",
    "                K (str): action\n",
    "                V (float): score\n",
    "        '''\n",
    "\n",
    "        a_inv = self.model['act_inv']\n",
    "        theta = self.model['theta']\n",
    "\n",
    "        estimated_reward = {}\n",
    "        uncertainty = {}\n",
    "        score_dict = {}\n",
    "        max_score = 0\n",
    "        for action_id in range(len(self.actions)):\n",
    "            action_context = np.reshape(context[action_id], (-1, 1))\n",
    "            estimated_reward[action_id] = float(theta[action_id].T.dot(action_context))\n",
    "            uncertainty[action_id] = float(self.alpha * np.sqrt(action_context.T.dot(a_inv[action_id]).dot(action_context)))\n",
    "            score_dict[action_id] = estimated_reward[action_id] + uncertainty[action_id]\n",
    "\n",
    "        return score_dict\n",
    "\n",
    "    def update(self, reward):\n",
    "        '''\n",
    "        Args:\n",
    "            reward (float)\n",
    "\n",
    "        Summary:\n",
    "            Updates self.model according to self.prev_context, self.prev_action, @reward.\n",
    "        '''\n",
    "        action_id = self.actions.index(self.prev_action)\n",
    "        action_context = np.reshape(self.prev_context[action_id], (-1, 1))\n",
    "        self.model['act'][action_id] += action_context.dot(action_context.T)\n",
    "        self.model['act_inv'][action_id] = np.linalg.inv(self.model['act'][action_id])\n",
    "        self.model['b'][action_id] += reward * action_context\n",
    "        self.model['theta'][action_id] = self.model['act_inv'][action_id].dot(self.model['b'][action_id])\n",
    "\n",
    "    def act(self, context, reward):\n",
    "        '''\n",
    "        Args:\n",
    "            context (iterable)\n",
    "            reward (float)\n",
    "\n",
    "        Returns:\n",
    "            (str): action.\n",
    "        '''\n",
    "\n",
    "        # Update previous context-action pair.\n",
    "        if self.prev_action is not None:\n",
    "            self.update(reward)\n",
    "\n",
    "        # Compute score.\n",
    "        context = self._pre_process_context(context)\n",
    "        score = self._compute_score(context)\n",
    "\n",
    "        # Compute best action.\n",
    "        best_action = np.random.choice(self.actions)\n",
    "        max_score = float(\"-inf\")\n",
    "        for action_id in range(len(self.actions)):\n",
    "            if score[action_id] > max_score:\n",
    "                max_score = score[action_id]\n",
    "                best_action = self.actions[action_id]\n",
    "\n",
    "\n",
    "        # Update prev pointers.\n",
    "        self.prev_action = best_action\n",
    "        self.prev_context = context\n",
    "        self.step_number += 1\n",
    "        \n",
    "        return best_action\n",
    "\n",
    "    def _pre_process_context(self, context):\n",
    "        if context.get_num_feats() == 1:\n",
    "            # If there's no context (that is, we're just in a regular bandit).\n",
    "            context = context.features()\n",
    "\n",
    "        if not hasattr(context[0], '__iter__'):\n",
    "            # If we only have a single context.\n",
    "            new_context = {}\n",
    "            for action_id in range(len(self.actions)):\n",
    "                new_context[action_id] = context\n",
    "            context = new_context\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"DC1\",\"DC2\",\"DC3\"]#[0,1,2]\n",
    "test=LinUCBAgent(actions, name=\"LinUCB\", rand_init=True, context_size=4, alpha=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'rand_init': True, 'context_size': 4, 'alpha': 1.5})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DC3'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python imports\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "''' StateClass.py: Contains the State Class. '''\n",
    "\n",
    "class State(Sequence):\n",
    "    ''' Abstract State class '''\n",
    "\n",
    "    def __init__(self, data=[], is_terminal=False):\n",
    "        self.data = data\n",
    "        self._is_terminal = is_terminal\n",
    "\n",
    "    def features(self):\n",
    "        '''\n",
    "        Summary\n",
    "            Used by function approximators to represent the state.\n",
    "            Override this method in State subclasses to have functiona\n",
    "            approximators use a different set of features.\n",
    "        Returns:\n",
    "            (iterable)\n",
    "        '''\n",
    "        return np.array(self.data).flatten()\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def get_num_feats(self):\n",
    "        return len(self.features())\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self._is_terminal\n",
    "\n",
    "    def set_terminal(self, is_term=True):\n",
    "        self._is_terminal = is_term\n",
    "\n",
    "    def __hash__(self):\n",
    "        if type(self.data).__module__ == np.__name__:\n",
    "            # Numpy arrays\n",
    "            return hash(str(self.data))\n",
    "        elif self.data.__hash__ is None:\n",
    "            return hash(tuple(self.data))\n",
    "        else:\n",
    "            return hash(self.data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"s.\" + str(self.data)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, State):\n",
    "            return self.data == other.data\n",
    "        return False\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "feat = State([1,0,0,1])\n",
    "feat.features()\n",
    "\n",
    "arms = [\n",
    "    np.array([1.0, 0.0, 0.0,1.0]),\n",
    "    np.array([0.0, 1.0, 0.0,0.0]),\n",
    "    np.array([0.0, 0.0, 1.0,1.0])\n",
    "]\n",
    "\n",
    "feat2=State(arms)\n",
    "test.act(feat2,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = feat2\n",
    "np.reshape(context[1], (-1, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_context = np.reshape(context[1], (-1, 1))\n",
    "action_context.dot(action_context.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat2.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC3\n",
      "DC3\n",
      "DC3\n",
      "DC3\n",
      "DC3\n",
      "DC3\n",
      "DC3\n",
      "DC3\n",
      "DC3\n",
      "DC3\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(test.act(feat2,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    test.update(10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': {0: array([[6., 0., 0., 5.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [5., 0., 0., 6.]]),\n",
       "  1: array([[1., 0., 0., 0.],\n",
       "         [0., 2., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]),\n",
       "  2: array([[ 1.,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 25., 24.],\n",
       "         [ 0.,  0., 24., 25.]])},\n",
       " 'act_inv': {0: array([[ 0.54545455,  0.        ,  0.        , -0.45454545],\n",
       "         [ 0.        ,  1.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  1.        ,  0.        ],\n",
       "         [-0.45454545,  0.        ,  0.        ,  0.54545455]]),\n",
       "  1: array([[1. , 0. , 0. , 0. ],\n",
       "         [0. , 0.5, 0. , 0. ],\n",
       "         [0. , 0. , 1. , 0. ],\n",
       "         [0. , 0. , 0. , 1. ]]),\n",
       "  2: array([[ 1.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  1.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.51020408, -0.48979592],\n",
       "         [ 0.        ,  0.        , -0.48979592,  0.51020408]])},\n",
       " 'theta': {0: array([[0.45454545],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.45454545]]),\n",
       "  1: array([[0. ],\n",
       "         [0.5],\n",
       "         [0. ],\n",
       "         [0. ]]),\n",
       "  2: array([[0.        ],\n",
       "         [0.        ],\n",
       "         [2.32653061],\n",
       "         [2.32653061]])},\n",
       " 'b': {0: array([[5.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [5.]]),\n",
       "  1: array([[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       "  2: array([[  0.],\n",
       "         [  0.],\n",
       "         [114.],\n",
       "         [114.]])}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f06f4baf162c1bba868b3ed890ca059fd0c13f705dc9d48413e36c65b86dd60d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
