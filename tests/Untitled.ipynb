{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c28bfad-9218-4d8d-8dc4-45fcec805c45",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     81\u001b[0m obs, prev_obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m190000\u001b[39m):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m#env.render()\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     d_obs \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     87\u001b[0m         action, action_prob \u001b[38;5;241m=\u001b[39m policy(d_obs)\n",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mPolicy.pre_process\u001b[0;34m(self, x, prev_x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpre_process\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, prev_x):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_to_tensor(prev_x)\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mPolicy.state_to_tensor\u001b[0;34m(self, I)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6000\u001b[39m)\n\u001b[1;32m     25\u001b[0m I \u001b[38;5;241m=\u001b[39m I[\u001b[38;5;241m35\u001b[39m:\u001b[38;5;241m185\u001b[39m] \u001b[38;5;66;03m# crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m I \u001b[38;5;241m=\u001b[39m \u001b[43mI\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# downsample by factor of 2.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m I[I \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m144\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# erase background (background type 1)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m I[I \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m109\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# erase background (background type 2)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.eps_clip = 0.1\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(6000, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def state_to_tensor(self, I):\n",
    "        \"\"\" prepro 210x160x3 uint8 frame into 6000 (75x80) 1D float vector. See Karpathy's post: http://karpathy.github.io/2016/05/31/rl/ \"\"\"\n",
    "        if I is None:\n",
    "            return torch.zeros(1, 6000)\n",
    "        I = I[35:185] # crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\n",
    "        I = I[::2,::2,0] # downsample by factor of 2.\n",
    "        I[I == 144] = 0 # erase background (background type 1)\n",
    "        I[I == 109] = 0 # erase background (background type 2)\n",
    "        I[I != 0] = 1 # everything else (paddles, ball) just set to 1. this makes the image grayscale effectively\n",
    "        return torch.from_numpy(I.astype(np.float32).ravel()).unsqueeze(0)\n",
    "\n",
    "    def pre_process(self, x, prev_x):\n",
    "        return self.state_to_tensor(x) - self.state_to_tensor(prev_x)\n",
    "\n",
    "    def convert_action(self, action):\n",
    "        return action + 2\n",
    "\n",
    "    def forward(self, d_obs, action=None, action_prob=None, advantage=None, deterministic=False):\n",
    "        if action is None:\n",
    "            with torch.no_grad():\n",
    "                logits = self.layers(d_obs)\n",
    "                if deterministic:\n",
    "                    action = int(torch.argmax(logits[0]).detach().cpu().numpy())\n",
    "                    action_prob = 1.0\n",
    "                else:\n",
    "                    c = torch.distributions.Categorical(logits=logits)\n",
    "                    action = int(c.sample().cpu().numpy()[0])\n",
    "                    action_prob = float(c.probs[0, action].detach().cpu().numpy())\n",
    "                return action, action_prob\n",
    "        '''\n",
    "        # policy gradient (REINFORCE)\n",
    "        logits = self.layers(d_obs)\n",
    "        loss = F.cross_entropy(logits, action, reduction='none') * advantage\n",
    "        return loss.mean()\n",
    "        '''\n",
    "\n",
    "        # PPO\n",
    "        vs = np.array([[1., 0.], [0., 1.]])\n",
    "        ts = torch.FloatTensor(vs[action.cpu().numpy()])\n",
    "\n",
    "        logits = self.layers(d_obs)\n",
    "        r = torch.sum(F.softmax(logits, dim=1) * ts, dim=1) / action_prob\n",
    "        loss1 = r * advantage\n",
    "        loss2 = torch.clamp(r, 1-self.eps_clip, 1+self.eps_clip) * advantage\n",
    "        loss = -torch.min(loss1, loss2)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env.reset()\n",
    "\n",
    "policy = Policy()\n",
    "\n",
    "opt = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "reward_sum_running_avg = None\n",
    "for it in range(100000):\n",
    "    d_obs_history, action_history, action_prob_history, reward_history = [], [], [], []\n",
    "    for ep in range(10):\n",
    "        obs, prev_obs = env.reset(), None\n",
    "        for t in range(190000):\n",
    "            #env.render()\n",
    "\n",
    "            d_obs = policy.pre_process(obs, prev_obs)\n",
    "            with torch.no_grad():\n",
    "                action, action_prob = policy(d_obs)\n",
    "\n",
    "            prev_obs = obs\n",
    "            obs, reward, done, info = env.step(policy.convert_action(action))\n",
    "\n",
    "            d_obs_history.append(d_obs)\n",
    "            action_history.append(action)\n",
    "            action_prob_history.append(action_prob)\n",
    "            reward_history.append(reward)\n",
    "\n",
    "            if done:\n",
    "                reward_sum = sum(reward_history[-t:])\n",
    "                reward_sum_running_avg = 0.99*reward_sum_running_avg + 0.01*reward_sum if reward_sum_running_avg else reward_sum\n",
    "                print('Iteration %d, Episode %d (%d timesteps) - last_action: %d, last_action_prob: %.2f, reward_sum: %.2f, running_avg: %.2f' % (it, ep, t, action, action_prob, reward_sum, reward_sum_running_avg))\n",
    "                break\n",
    "\n",
    "    # compute advantage\n",
    "    R = 0\n",
    "    discounted_rewards = []\n",
    "\n",
    "    for r in reward_history[::-1]:\n",
    "        if r != 0: R = 0 # scored/lost a point in pong, so reset reward sum\n",
    "        R = r + policy.gamma * R\n",
    "        discounted_rewards.insert(0, R)\n",
    "\n",
    "    discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / discounted_rewards.std()\n",
    "\n",
    "    # update policy\n",
    "    for _ in range(5):\n",
    "        n_batch = 24576\n",
    "        idxs = random.sample(range(len(action_history)), n_batch)\n",
    "        d_obs_batch = torch.cat([d_obs_history[idx] for idx in idxs], 0)\n",
    "        action_batch = torch.LongTensor([action_history[idx] for idx in idxs])\n",
    "        action_prob_batch = torch.FloatTensor([action_prob_history[idx] for idx in idxs])\n",
    "        advantage_batch = torch.FloatTensor([discounted_rewards[idx] for idx in idxs])\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss = policy(d_obs_batch, action_batch, action_prob_batch, advantage_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    if it % 5 == 0:\n",
    "        torch.save(policy.state_dict(), 'params.ckpt')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cb5c7e-35db-4a53-8b01-7dd3b979c640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvChecker<AtariEnv<PongNoFrameskip-v4>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
