{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cec7a95-fb07-4cef-a5f0-60db2b49fcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ppo2.py\n"
     ]
    }
   ],
   "source": [
    "%%file ppo2.py\n",
    "import argparse\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Solve the Pendulum-v0 with PPO')\n",
    "parser.add_argument(\n",
    "    '--gamma', type=float, default=0.9, metavar='G', help='discount factor (default: 0.9)')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='N', help='random seed (default: 0)')\n",
    "parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument(\n",
    "    '--log-interval',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    metavar='N',\n",
    "    help='interval between training status logs (default: 10)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
    "Transition = namedtuple('Transition', ['s', 'a', 'a_log_p', 'r', 's_'])\n",
    "\n",
    "import numpy as np\n",
    "import math,random\n",
    "class ActorNet2(object):\n",
    "  \n",
    "  def __init__(self, input_size, hidden_size,output_size, std=5e-1):\n",
    "    \n",
    "    print(\"An actor network is created.\")\n",
    "    \n",
    "    self.params = {}\n",
    "    self.params['W1'] = self._uniform_init(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['W2'] = self._uniform_init(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "    self.params['W3'] = self._uniform_init(hidden_size, output_size)\n",
    "    self.params['b3'] = np.zeros(output_size)\n",
    "    \n",
    "    self.optm_cfg ={}\n",
    "    self.optm_cfg['W1'] = None\n",
    "    self.optm_cfg['b1'] = None\n",
    "    self.optm_cfg['W2'] = None\n",
    "    self.optm_cfg['b2'] = None\n",
    "    self.optm_cfg['W3'] = None\n",
    "    self.optm_cfg['b3'] = None\n",
    "    \n",
    "\n",
    "\n",
    "  def evaluate_gradient(self, s, a, olp, adv,b, clip_param, max_norm):\n",
    "    \n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    W3, b3 = self.params['W3'], self.params['b3']\n",
    "        \n",
    "    batch_size, _ = s.shape\n",
    "  \n",
    "    z1=np.dot(s,W1)+b1\n",
    "    H1=np.maximum(0,z1) \n",
    "    z2=np.dot(H1,W2)+b2\n",
    "    mu=b * np.tanh(z2) \n",
    "\n",
    "    z3=np.dot(H1,W3)+b3\n",
    "    sigma=np.log(1+np.exp(z3))\n",
    "    \n",
    "    alp=np.zeros((batch_size,len(a[0])))\n",
    "    ratio=np.zeros((batch_size,len(a[0])))\n",
    "    surr1=np.zeros((batch_size,len(a[0])))\n",
    "    surr2=np.zeros((batch_size,len(a[0])))\n",
    "    mu_derv=np.zeros((batch_size,len(a[0])))\n",
    "    sigma_derv=np.zeros((batch_size,len(a[0])))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(len(a[0])):\n",
    "            alp[i,j]=-((a[i,j] - mu[i,j]) ** 2) / (2 * sigma[i,j]**2) - np.log(sigma[i,j]) - np.log(np.sqrt(2 * np.pi))\n",
    "            ratio[i,j]= np.exp(alp[i,j]-olp[i,j])\n",
    "    \n",
    "            surr1[i,j]=ratio[i,j]*adv[i]\n",
    "            surr2[i,j]= np.clip(ratio[i,j],1-clip_param,1+clip_param)*adv[i]\n",
    "                    \n",
    "            if(surr2[i,j]<surr1[i,j] and (ratio[i,j]<1-clip_param or ratio[i,j]>1+clip_param)):\n",
    "                mu_derv[i,j]=0\n",
    "                sigma_derv[i,j]=0\n",
    "            else:\n",
    "                mu_derv[i,j]=-(b*adv[i]*math.exp(-(a[i,j]-b*math.tanh(z2[i,j]))**2/(2*sigma[i,j]**2)-olp[i,j])*(1-(math.tanh(z2[i,j]))**2)*(a[i,j]-b*math.tanh(z2[i,j])))/(math.sqrt(2*math.pi)*sigma[i,j]**3)\n",
    "                sigma_derv[i,j]=(adv[i]*math.exp(-(a[i,j]-mu[i,j])**2/(2*math.log(math.exp(z3[i,j])+1)**2)+z3[i,j]-olp[i,j])*(math.log(math.exp(z3[i,j])+1)**2-mu[i,j]**2+2*a[i,j]*mu[i,j]-a[i,j]**2))/(math.sqrt(2*math.pi)*(math.exp(z3[i,j])+1)*math.log(math.exp(z3[i,j])+1)**4)\n",
    "        \n",
    "            \n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    out1=sigma_derv.dot(W3.T)\n",
    "    out1+=mu_derv.dot(W2.T)\n",
    "    \n",
    "    out1[z1<=0]=0\n",
    "\n",
    "    sigma_derv/=len(a[0])\n",
    "    mu_derv/=len(a[0])\n",
    "    grads['W3']=np.dot(H1.T, sigma_derv)/batch_size\n",
    "    grads['W2']=np.dot(H1.T, mu_derv)/batch_size\n",
    "    grads['W1']=np.dot(s.T, out1)/batch_size\n",
    "    grads['b3']=np.sum(sigma_derv, axis=0)/batch_size\n",
    "    grads['b2']=np.sum(mu_derv, axis=0)/batch_size\n",
    "    grads['b1']=np.sum(out1, axis=0)/batch_size\n",
    "    \n",
    "    \n",
    "    total_norm = np.sqrt((grads['W3']**2).sum()+(grads['W2']**2).sum()+(grads['W1']**2).sum()\n",
    "    +(grads['b3']**2).sum()+(grads['b2']**2).sum()+(grads['b1']**2).sum())\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        grads['W3']*=clip_coef\n",
    "        grads['W2']*=clip_coef\n",
    "        grads['W1']*=clip_coef\n",
    "        grads['b3']*=clip_coef\n",
    "        grads['b2']*=clip_coef\n",
    "        grads['b1']*=clip_coef\n",
    "    return  grads\n",
    "\n",
    "  def train(self, s, a, olp, adv,b, clip_param, max_grad_norm):\n",
    "     # Compute out and gradients using the current minibatch\n",
    "    grads = self.evaluate_gradient(s, a, olp, adv,b, clip_param, max_grad_norm)\n",
    "    # Update the weights using adam optimizer\n",
    "    \n",
    "    self.params['W3'] = self._adam(self.params['W3'], grads['W3'], config=self.optm_cfg['W3'])[0]\n",
    "    self.params['W2'] = self._adam(self.params['W2'], grads['W2'], config=self.optm_cfg['W2'])[0]\n",
    "    self.params['W1'] = self._adam(self.params['W1'], grads['W1'], config=self.optm_cfg['W1'])[0]\n",
    "    self.params['b3'] = self._adam(self.params['b3'], grads['b3'], config=self.optm_cfg['b3'])[0]\n",
    "    self.params['b2'] = self._adam(self.params['b2'], grads['b2'], config=self.optm_cfg['b2'])[0]\n",
    "    self.params['b1'] = self._adam(self.params['b1'], grads['b1'], config=self.optm_cfg['b1'])[0]\n",
    "    \n",
    "    # Update the configuration parameters to be used in the next iteration\n",
    "    self.optm_cfg['W3'] = self._adam(self.params['W3'], grads['W3'], config=self.optm_cfg['W3'])[1]\n",
    "    self.optm_cfg['W2'] = self._adam(self.params['W2'], grads['W2'], config=self.optm_cfg['W2'])[1]\n",
    "    self.optm_cfg['W1'] = self._adam(self.params['W1'], grads['W1'], config=self.optm_cfg['W1'])[1]\n",
    "    self.optm_cfg['b3'] = self._adam(self.params['b3'], grads['b3'], config=self.optm_cfg['b3'])[1]\n",
    "    self.optm_cfg['b2'] = self._adam(self.params['b2'], grads['b2'], config=self.optm_cfg['b2'])[1]\n",
    "    self.optm_cfg['b1'] = self._adam(self.params['b1'], grads['b1'], config=self.optm_cfg['b1'])[1]\n",
    "\n",
    "    \n",
    "  def predict(self, s, b):\n",
    "\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    W3, b3 = self.params['W3'], self.params['b3']\n",
    "    \n",
    "    z1=np.dot(s,W1)+b1\n",
    "    H1=np.maximum(0,z1) \n",
    "    z2=np.dot(H1,W2)+b2\n",
    "    mu=b * np.tanh(z2) \n",
    "\n",
    "    z3=np.dot(H1,W3)+b3\n",
    "    sigma=np.log(1+np.exp(z3))\n",
    "    \n",
    "    mu=np.array([mu])\n",
    "    sigma=np.array([sigma])\n",
    "    \n",
    "    a=np.zeros((1,2))\n",
    "    alp=np.zeros((1,2))\n",
    "    for j in range(2):\n",
    "        a[0,j] = mu[0,j] + sigma[0,j] * math.sqrt(-2.0 * math.log(random.random())) *math.sin(2.0 * math.pi * random.random())\n",
    "        alp[0,j] =-((a[0,j] - mu[0,j]) ** 2) / (2 * sigma[0,j] ** 2) - np.log(sigma[0,j]) - math.log(math.sqrt(2 * math.pi))\n",
    "        \n",
    "    return a,alp\n",
    "\n",
    "  def _adam(self, x, dx, config=None):\n",
    "      if config is None: config = {}\n",
    "      config.setdefault('learning_rate', 1e-4)\n",
    "      config.setdefault('beta1', 0.9)\n",
    "      config.setdefault('beta2', 0.999)\n",
    "      config.setdefault('epsilon', 1e-8)\n",
    "      config.setdefault('m', np.zeros_like(x))\n",
    "      config.setdefault('v', np.zeros_like(x))\n",
    "      config.setdefault('t', 0)\n",
    "      \n",
    "      next_x = None\n",
    "      \n",
    "      #Adam update formula,                                                 #\n",
    "      config['t'] += 1\n",
    "      config['m'] = config['beta1']*config['m'] + (1-config['beta1'])*dx\n",
    "      config['v'] = config['beta2']*config['v'] + (1-config['beta2'])*(dx**2)\n",
    "      mb = config['m'] / (1 - config['beta1']**config['t'])\n",
    "      vb = config['v'] / (1 - config['beta2']**config['t'])\n",
    "    \n",
    "      next_x = x - config['learning_rate'] * mb / (np.sqrt(vb) + config['epsilon'])\n",
    "      return next_x, config\n",
    "  \n",
    "  def _uniform_init(self, input_size, output_size):\n",
    "      u = np.sqrt(1./(input_size*output_size))\n",
    "      return np.random.uniform(-u, u, (input_size, output_size))\n",
    "\n",
    "\n",
    "class CriticNet2(object):\n",
    "  \n",
    "  def __init__(self, input_size, hidden_size,output_size, std=5e-1):\n",
    "    \n",
    "    print(\"An actor network is created.\")\n",
    "    \n",
    "    self.params = {}\n",
    "    self.params['W1'] = self._uniform_init(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['W2'] = self._uniform_init(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    self.optm_cfg ={}\n",
    "    self.optm_cfg['W1'] = None\n",
    "    self.optm_cfg['b1'] = None\n",
    "    self.optm_cfg['W2'] = None\n",
    "    self.optm_cfg['b2'] = None\n",
    "    \n",
    "\n",
    "\n",
    "  def evaluate_gradient(self, s, target_v, max_norm):\n",
    "    \n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    \n",
    "    batch_size, _ = s.shape\n",
    "  \n",
    "    z1=np.dot(s,W1)+b1\n",
    "    H1=np.maximum(0,z1) \n",
    "    v=np.dot(H1,W2)+b2\n",
    "    \n",
    "    d=np.zeros((batch_size,1))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        d[i,0]=v[i,0]-target_v[i]\n",
    "        if(d[i,0]<-1):\n",
    "            d[i,0]=-1\n",
    "        elif(d[i,0]>1):\n",
    "            d[i,0]=1\n",
    "            \n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    out1=d.dot(W2.T)\n",
    "\n",
    "    out1[z1<=0]=0\n",
    "\n",
    "    grads['W2']=np.dot(H1.T, d)/batch_size\n",
    "    grads['W1']=np.dot(s.T, out1)/batch_size\n",
    "    grads['b2']=np.sum(d, axis=0)/batch_size\n",
    "    grads['b1']=np.sum(out1, axis=0)/batch_size\n",
    "    \n",
    "    total_norm = np.sqrt((grads['W2']**2).sum()+(grads['W1']**2).sum()\n",
    "    +(grads['b2']**2).sum()+(grads['b1']**2).sum())\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        grads['W2']*=clip_coef\n",
    "        grads['W1']*=clip_coef\n",
    "        grads['b2']*=clip_coef\n",
    "        grads['b1']*=clip_coef\n",
    "    return  grads\n",
    "\n",
    "  def train(self, s, target_v, max_grad_norm):\n",
    "     # Compute out and gradients using the current minibatch\n",
    "    grads = self.evaluate_gradient(s, target_v, max_grad_norm)\n",
    "    # Update the weights using adam optimizer\n",
    "    \n",
    "    self.params['W2'] = self._adam(self.params['W2'], grads['W2'], config=self.optm_cfg['W2'])[0]\n",
    "    self.params['W1'] = self._adam(self.params['W1'], grads['W1'], config=self.optm_cfg['W1'])[0]\n",
    "    self.params['b2'] = self._adam(self.params['b2'], grads['b2'], config=self.optm_cfg['b2'])[0]\n",
    "    self.params['b1'] = self._adam(self.params['b1'], grads['b1'], config=self.optm_cfg['b1'])[0]\n",
    "    \n",
    "    # Update the configuration parameters to be used in the next iteration\n",
    "    self.optm_cfg['W2'] = self._adam(self.params['W2'], grads['W2'], config=self.optm_cfg['W2'])[1]\n",
    "    self.optm_cfg['W1'] = self._adam(self.params['W1'], grads['W1'], config=self.optm_cfg['W1'])[1]\n",
    "    self.optm_cfg['b2'] = self._adam(self.params['b2'], grads['b2'], config=self.optm_cfg['b2'])[1]\n",
    "    self.optm_cfg['b1'] = self._adam(self.params['b1'], grads['b1'], config=self.optm_cfg['b1'])[1]\n",
    "\n",
    "    \n",
    "  def predict(self, s):\n",
    "\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    \n",
    "    z1=np.dot(s,W1)+b1\n",
    "    H1=np.maximum(0,z1) \n",
    "    v=np.dot(H1,W2)+b2\n",
    "\n",
    "    return v\n",
    "\n",
    "  def _adam(self, x, dx, config=None):\n",
    "      if config is None: config = {}\n",
    "      config.setdefault('learning_rate', 3e-4)\n",
    "      config.setdefault('beta1', 0.9)\n",
    "      config.setdefault('beta2', 0.999)\n",
    "      config.setdefault('epsilon', 1e-8)\n",
    "      config.setdefault('m', np.zeros_like(x))\n",
    "      config.setdefault('v', np.zeros_like(x))\n",
    "      config.setdefault('t', 0)\n",
    "      \n",
    "      next_x = None\n",
    "      \n",
    "      #Adam update formula,                                                 #\n",
    "      config['t'] += 1\n",
    "      config['m'] = config['beta1']*config['m'] + (1-config['beta1'])*dx\n",
    "      config['v'] = config['beta2']*config['v'] + (1-config['beta2'])*(dx**2)\n",
    "      mb = config['m'] / (1 - config['beta1']**config['t'])\n",
    "      vb = config['v'] / (1 - config['beta2']**config['t'])\n",
    "    \n",
    "      next_x = x - config['learning_rate'] * mb / (np.sqrt(vb) + config['epsilon'])\n",
    "      return next_x, config\n",
    "  \n",
    "  def _uniform_init(self, input_size, output_size):\n",
    "      u = np.sqrt(1./(input_size*output_size))\n",
    "      return np.random.uniform(-u, u, (input_size, output_size))\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    clip_param = 0.2\n",
    "    max_grad_norm = 0.5\n",
    "    ppo_epoch = 10\n",
    "    buffer_capacity, batch_size = 1000, 32\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_step = 0\n",
    "        self.manet = ActorNet2(3,100,2)\n",
    "        self.mcnet = CriticNet2(3,100,1)\n",
    "        self.buffer = []\n",
    "        self.counter = 0   \n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        self.counter += 1\n",
    "        return self.counter % self.buffer_capacity == 0\n",
    "\n",
    "    def update(self):\n",
    "        self.training_step += 1\n",
    "\n",
    "        bs = np.array([t.s for t in self.buffer])\n",
    "        d = [t.a for t in self.buffer]\n",
    "        ba = np.array([[a[0,0],a[0,1]] for a in d])\n",
    "        br = np.array([t.r for t in self.buffer])\n",
    "        bs1 = np.array([t.s_ for t in self.buffer])\n",
    "        \n",
    "        d = [t.a_log_p for t in self.buffer]\n",
    "        bolp = np.array([[a[0,0],a[0,1]] for a in d])\n",
    "\n",
    "        n=len(bs)\n",
    "        k=self.batch_size\n",
    "            \n",
    "        mean=0\n",
    "        for i in range(n):\n",
    "            mean+=br[i]\n",
    "        mean/=n\n",
    "        sum=0\n",
    "        for i in range(n):\n",
    "            sum+=(br[i]-mean)**2\n",
    "        std=math.sqrt(sum/(n-1))\n",
    "        \n",
    "        target_v = np.zeros((n))\n",
    "        adv = np.zeros((n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            br[i]=(br[i]-mean)/(std+1e-5)\n",
    "            \n",
    "            target_v[i]=br[i] + args.gamma * self.mcnet.predict(bs1)[i,0]\n",
    "            adv[i] = target_v[i] - self.mcnet.predict(bs)[i,0]\n",
    "        \n",
    "        for _ in range(self.ppo_epoch*32):\n",
    "            \n",
    "            pool=np.zeros((n))\n",
    "            result=np.zeros((k)).astype(int)\n",
    "            for i in range(n):\n",
    "                pool[i]=i\n",
    "            for i in range(k):\n",
    "                j = random.randint(0,n - i-1)\n",
    "                result[i] = pool[j]\n",
    "                pool[j] = pool[n - i - 1]\n",
    "            \n",
    "            s=np.zeros((k,len(bs[0])))\n",
    "            a=np.zeros((k,len(ba[0])))\n",
    "            s1=np.zeros((k,len(bs[0])))\n",
    "            olp=np.zeros((k,len(ba[0])))\n",
    "            target_v0=np.zeros((k))\n",
    "            adv0=np.zeros((k))\n",
    "            for i in range(k):\n",
    "                target_v0[i]=target_v[result[i]] \n",
    "                adv0[i]=adv[result[i]] \n",
    "                for j in range(len(bs[0])):\n",
    "                    s[i,j]=bs[result[i],j] \n",
    "                for j in range(len(ba[0])):\n",
    "                    a[i,j]=ba[result[i],j] \n",
    "                    olp[i,j]=bolp[result[i],j]\n",
    "                \n",
    "            self.manet.train(s, a, olp, adv0, 2.0, self.clip_param, self.max_grad_norm)\n",
    "            self.mcnet.train(s,target_v0, self.max_grad_norm)\n",
    "            \n",
    "        del self.buffer[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    #env.seed(args.seed)\n",
    "\n",
    "    agent = Agent()\n",
    "\n",
    "    training_records = []\n",
    "    running_reward = -1000\n",
    "    state,_ = env.reset()\n",
    "    for i_ep in range(1):\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(2000000000):\n",
    "            action, action_log_prob = agent.manet.predict(state,2.0)\n",
    "            state_, reward, done, _,_ = env.step([action[0,0].item()])\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            if agent.store(Transition(state, action, action_log_prob, (reward + 8) / 8, state_)):\n",
    "                agent.update()\n",
    "            score += reward\n",
    "            state = state_\n",
    "\n",
    "            if(t%200==0):\n",
    "                running_reward = running_reward * 0.9 + score * 0.1\n",
    "                training_records.append(TrainingRecord(i_ep, running_reward))\n",
    "            if(t%1000==0):\n",
    "                print('Ep {}\\tMoving average score: {:.2f}\\t'.format((int)(t/1000), running_reward))\n",
    "                score=0\n",
    "\n",
    "            if running_reward > -200:\n",
    "                print(\"Solved! Moving average score is now {}!\".format(running_reward))\n",
    "                env.close()\n",
    "                \n",
    "                break\n",
    "\n",
    "    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
    "    plt.title('PPO')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving averaged episode reward')\n",
    "    plt.savefig(\"ppo.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8ceb2c-3252-4535-8e28-059a373a9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=[1,2,3]\n",
    "del memory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893f92db-897f-4a8d-961b-4bbd8ee4f39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb0f3c7-b9fa-462b-a70b-f2f40ad060d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_over=True \n",
    "~game_over"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
