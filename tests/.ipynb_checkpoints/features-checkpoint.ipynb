{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c268ff-b95b-4f93-ba66-916d13b38e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class OneHotEncoder:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Convert between category labels and their one-hot vector\n",
    "        representations.\n",
    "        Parameters\n",
    "        ----------\n",
    "        categories : list of length `C`\n",
    "            List of the unique category labels for the items to encode.\n",
    "        \"\"\"\n",
    "        self._is_fit = False\n",
    "        self.hyperparameters = {}\n",
    "        self.parameters = {\"categories\": None}\n",
    "\n",
    "    def __call__(self, labels):\n",
    "        return self.transform(labels)\n",
    "\n",
    "    def fit(self, categories):\n",
    "        \"\"\"\n",
    "        Create mappings between columns and category labels.\n",
    "        Parameters\n",
    "        ----------\n",
    "        categories : list of length `C`\n",
    "            List of the unique category labels for the items to encode.\n",
    "        \"\"\"\n",
    "        self.parameters[\"categories\"] = categories\n",
    "        self.cat2idx = {c: i for i, c in enumerate(categories)}\n",
    "        self.idx2cat = {i: c for i, c in enumerate(categories)}\n",
    "        self._is_fit = True\n",
    "\n",
    "    def transform(self, labels, categories=None):\n",
    "        \"\"\"\n",
    "        Convert a list of labels into a one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : list of length `N`\n",
    "            A list of category labels.\n",
    "        categories : list of length `C`\n",
    "            List of the unique category labels for the items to encode. Default\n",
    "            is None.\n",
    "        Returns\n",
    "        -------\n",
    "        Y : :py:class:`ndarray <numpy.ndarray>` of shape `(N, C)`\n",
    "            The one-hot encoded labels. Each row corresponds to an example,\n",
    "            with a single 1 in the column corresponding to the respective\n",
    "            label.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            categories = set(labels) if categories is None else categories\n",
    "            self.fit(categories)\n",
    "\n",
    "        unknown = list(set(labels) - set(self.cat2idx.keys()))\n",
    "        assert len(unknown) == 0, \"Unrecognized label(s): {}\".format(unknown)\n",
    "\n",
    "        N, C = len(labels), len(self.cat2idx)\n",
    "        cols = np.array([self.cat2idx[c] for c in labels])\n",
    "\n",
    "        Y = np.zeros((N, C))\n",
    "        Y[np.arange(N), cols] = 1\n",
    "        return Y\n",
    "\n",
    "    def inverse_transform(self, Y):\n",
    "        \"\"\"\n",
    "        Convert a one-hot encoding back into the corresponding labels\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : :py:class:`ndarray <numpy.ndarray>` of shape `(N, C)`\n",
    "            One-hot encoded labels. Each row corresponds to an example, with a\n",
    "            single 1 in the column associated with the label for that example\n",
    "        Returns\n",
    "        -------\n",
    "        labels : list of length `N`\n",
    "            The list of category labels corresponding to the nonzero columns in\n",
    "            `Y`\n",
    "        \"\"\"\n",
    "        C = len(self.cat2idx)\n",
    "        assert Y.ndim == 2, \"Y must be 2D, but has shape {}\".format(Y.shape)\n",
    "        assert Y.shape[1] == C, \"Y must have {} columns, got {}\".format(C, Y.shape[1])\n",
    "        return [self.idx2cat[ix] for ix in Y.nonzero()[1]]\n",
    "    \n",
    "test=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76af4b11-005a-45a5-8960-5ae7ddf4dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fit([\"DC1\",\"DC2\",\"DC20\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49992c3d-c3b5-4061-abca-6d461a6cd17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yu=test.transform([\"DC20\",\"DC1\"])\n",
    "yu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c82da12e-e747-4cb2-a57a-d914c4008d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DC20', 'DC1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.inverse_transform(yu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ac3934-aa20-4b57-90bd-de16f611cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=Standardizer()\n",
    "test2.fit(yu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "844d60e5-3651-4f4a-836e-bb5e5ae4f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/j2c4_5dx0cs30cq_jy0h_b0w0000gn/T/ipykernel_47443/1804379527.py:92: RuntimeWarning: invalid value encountered in divide\n",
      "  return (X - self._mean) / self._std\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1., nan,  1.],\n",
       "       [ 1., nan, -1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.transform(yu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa606ff-acd2-4aaf-8b9e-8a532ded5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    def __init__(self, with_mean=True, with_std=True):\n",
    "        \"\"\"\n",
    "        Feature-wise standardization for vector inputs.\n",
    "        Notes\n",
    "        -----\n",
    "        Due to the sensitivity of empirical mean and standard deviation\n",
    "        calculations to extreme values, `Standardizer` cannot guarantee\n",
    "        balanced feature scales in the presence of outliers. In particular,\n",
    "        note that because outliers for each feature can have different\n",
    "        magnitudes, the spread of the transformed data on each feature can be\n",
    "        very different.\n",
    "        Similar to sklearn, `Standardizer` uses a biased estimator for the\n",
    "        standard deviation: ``numpy.std(x, ddof=0)``.\n",
    "        Parameters\n",
    "        ----------\n",
    "        with_mean : bool\n",
    "            Whether to scale samples to have 0 mean during transformation.\n",
    "            Default is True.\n",
    "        with_std : bool\n",
    "            Whether to scale samples to have unit variance during\n",
    "            transformation. Default is True.\n",
    "        \"\"\"\n",
    "        self.with_mean = with_mean\n",
    "        self.with_std = with_std\n",
    "        self._is_fit = False\n",
    "\n",
    "    @property\n",
    "    def hyperparameters(self):\n",
    "        H = {\"with_mean\": self.with_mean, \"with_std\": self.with_std}\n",
    "        return H\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = {\n",
    "            \"mean\": self._mean if hasattr(self, \"mean\") else None,\n",
    "            \"std\": self._std if hasattr(self, \"std\") else None,\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Store the feature-wise mean and standard deviation across the samples\n",
    "        in `X` for future scaling.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, C)`\n",
    "            An array of N samples, each with dimensionality `C`\n",
    "        \"\"\"\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if X.shape[0] < 2:\n",
    "            raise ValueError(\"`X` must contain at least 2 samples\")\n",
    "\n",
    "        std = np.ones(X.shape[1])\n",
    "        mean = np.zeros(X.shape[1])\n",
    "\n",
    "        if self.with_mean:\n",
    "            mean = np.mean(X, axis=0)\n",
    "\n",
    "        if self.with_std:\n",
    "            std = np.std(X, axis=0, ddof=0)\n",
    "\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "        self._is_fit = True\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Standardize features by removing the mean and scaling to unit variance.\n",
    "        For a sample `x`, the standardized score is calculated as:\n",
    "        .. math::\n",
    "            z = (x - u) / s\n",
    "        where `u` is the mean of the training samples or zero if `with_mean` is\n",
    "        False, and `s` is the standard deviation of the training samples or 1\n",
    "        if `with_std` is False.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, C)`\n",
    "            An array of N samples, each with dimensionality `C`.\n",
    "        Returns\n",
    "        -------\n",
    "        Z : :py:class:`ndarray <numpy.ndarray>` of shape `(N, C)`\n",
    "            The feature-wise standardized version of `X`.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            raise Exception(\"Must call `fit` before using the `transform` method\")\n",
    "        return (X - self._mean) / self._std\n",
    "\n",
    "    def inverse_transform(self, Z):\n",
    "        \"\"\"\n",
    "        Convert a collection of standardized features back into the original\n",
    "        feature space.\n",
    "        For a standardized sample `z`, the unstandardized score is calculated as:\n",
    "        .. math::\n",
    "            x = z s + u\n",
    "        where `u` is the mean of the training samples or zero if `with_mean` is\n",
    "        False, and `s` is the standard deviation of the training samples or 1\n",
    "        if `with_std` is False.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : :py:class:`ndarray <numpy.ndarray>` of shape `(N, C)`\n",
    "            An array of `N` standardized samples, each with dimensionality `C`.\n",
    "        Returns\n",
    "        -------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, C)`\n",
    "            The unstandardixed samples from `Z`.\n",
    "        \"\"\"\n",
    "        assert self._is_fit, \"Must fit `Standardizer` before calling inverse_transform\"\n",
    "        P = self.parameters\n",
    "        mean, std = P[\"mean\"], P[\"std\"]\n",
    "        return Z * std + mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56eb8dbc-4578-446c-a154-c1f7b7b72d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"Discounted cumulative sum.\n",
    "    See https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html#difference-equation-filtering  # noqa: E501\n",
    "    Here, we have y[t] - discount*y[t+1] = x[t]\n",
    "    or rev(y)[t] - discount*rev(y)[t-1] = rev(x)[t]\n",
    "    Args:\n",
    "        x (np.ndarrary): Input.\n",
    "        discount (float): Discount factor.\n",
    "    Returns:\n",
    "        np.ndarrary: Discounted cumulative sum.\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1],\n",
    "                                axis=-1)[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acda73a5-24e5-4b38-ade9-34aa1f65d8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.6848, 4.88  , 3.    ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_cumsum([1,2,3],0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5ebc62e-44f3-46e7-8eb6-967a617fff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.6848, 4.88  , 3.    ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class dis:\n",
    "    def calculate_discounted_returns(self, rewards,gamma=0.96):\n",
    "        \"\"\"\n",
    "        Calculate discounted reward and then normalize it\n",
    "        (see Sutton book for definition)\n",
    "        Params:\n",
    "            rewards: list of rewards for every episode\n",
    "        \"\"\"\n",
    "        returns = np.zeros(len(rewards))\n",
    "    \n",
    "        next_return = 0 # 0 because we start at the last timestep\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            next_return = rewards[t] + gamma * next_return\n",
    "            returns[t] = next_return\n",
    "        # normalize for better statistical properties\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "        return returns\n",
    "    def _discount(self, x, gamma):\n",
    "        return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "    \n",
    "dis().calculate_discounted_returns([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d39e1b7-955e-456c-ace6-e39692610a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.6848, 4.88  , 3.    ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis()._discount([1,2,3],0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42a0ad86-9649-4cf6-acd8-1902caff7360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.96\n",
    "\n",
    "def discount_rewards(r, gamma):\n",
    "    if isinstance(r,list):\n",
    "        r=np.vstack(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # From the last reward to the first...\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        # ...reset the reward sum\n",
    "        if r[t] != 0: running_add = 0\n",
    "        # ...compute the discounted reward\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "r=[1,2,3]\n",
    "discount_rewards(r, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
