{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15878d84-3d7c-4781-9950-0030d2827563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,起点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5a7066-249f-450b-a76f-cd574c0f37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minirl.core.dynaQ import DynaQ\n",
    "import hirlite\n",
    "\n",
    "tdb = hirlite.Rlite(encoding='utf8',path=\"test.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9096f3ac-4f30-411f-acf7-2769ca0f5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynaQ_CliffWalking(n_planning):\n",
    "    ncol = 12\n",
    "    nrow = 4\n",
    "    env = CliffWalkingEnv(ncol, nrow)\n",
    "    epsilon = 0.01\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    state_space = [i for i in range(nrow * ncol)]\n",
    "    actions_list = [i for i in range(4)]\n",
    "    agent = DynaQ(state_space=state_space,\n",
    "        actions = actions_list,\n",
    "        alpha=0.7,\n",
    "        gamma=0.9,\n",
    "        random_seed=0,\n",
    "        eps=0.02,\n",
    "        model_db=tdb,\n",
    "        score_db=tdb,\n",
    "        his_db=tdb,\n",
    "        N=n_planning,  # no. of steps in planning phase\n",
    "        n=2,)\n",
    "    #agent = DynaQ(ncol, nrow, epsilon, alpha, gamma, n_planning)\n",
    "    num_episodes = 300  # 智能体在环境中运行多少条序列\n",
    "\n",
    "    return_list = []  # 记录每一条序列的回报\n",
    "    for i in range(10):  # 显示10个进度条\n",
    "        # tqdm的进度条功能\n",
    "        with tqdm(total=int(num_episodes / 10),\n",
    "                  desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n",
    "                episode_return = 0\n",
    "                state = env.reset()\n",
    "\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = agent.act(state,\"local_model\",\"local_model\",use_doubleQ=True)\n",
    "                    #print(agent.model)\n",
    "                    #print(actions_list)\n",
    "                    #action = random.choice(actions_list)\n",
    "                    action = int(action)\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    #print(next_state,reward,type(reward))\n",
    "                    episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n",
    "                    agent.learn(state, reward, \"local_model\",\"local_model\",use_doubleQ=True,use_dyna=True)\n",
    "                    #agent.update(state, action, reward, next_state,\"local_model\")\n",
    "                    model_id=\"local_model\"\n",
    "                    score_key1 = f\"{model_id}:{state}:Qscore1\"\n",
    "\n",
    "                    f=agent._score_db.zrange(score_key1,'0','0')\n",
    "                    #print(f,score_key1)\n",
    "                    state = next_state\n",
    "                return_list.append(episode_return)\n",
    "                if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n",
    "                    pbar.set_postfix({\n",
    "                        'episode':\n",
    "                        '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return':\n",
    "                        '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815b01e-1a2e-4f20-9501-5656426427a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-planning步数为：2\n",
      "simple two layer neural network based on numpy\n",
      "creating nn: #input:20 #hidden:64 #output:[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "n_planning_list = [2]\n",
    "for n_planning in n_planning_list:\n",
    "    print('Q-planning步数为：%d' % n_planning)\n",
    "    time.sleep(0.5)\n",
    "    return_list = DynaQ_CliffWalking(n_planning)\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    plt.plot(episodes_list,\n",
    "             return_list,\n",
    "             label=str(n_planning) + ' planning steps')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Dyna-Q on {}'.format('Cliff Walking'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ebd85a-1043-4707-90be-a785e29100a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
