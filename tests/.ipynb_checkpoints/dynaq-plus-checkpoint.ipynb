{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5fe5fb-dcc3-4e1f-b281-e33493e7dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,起点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68912b64-509f-4e1f-8df5-bb4a192f9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minirl import DynaQPlus\n",
    "from minirl.core.dynaQ_plus import TimeModel,simpleModel\n",
    "import hirlite\n",
    "\n",
    "mdb = hirlite.Rlite(encoding='utf8',path=\"model.db\")\n",
    "\n",
    "hdb = hirlite.Rlite(encoding='utf8',path=\"his.db\")\n",
    "sdb = hirlite.Rlite(encoding='utf8',path=\"score.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be503b5-1205-4e01-a525-e510c8e94063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynaQ_CliffWalking(n_planning):\n",
    "    ncol = 12\n",
    "    nrow = 4\n",
    "    env = CliffWalkingEnv(ncol, nrow)\n",
    "    epsilon = 0.01\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    state_space = [i for i in range(nrow * ncol)]\n",
    "    actions_list = [i for i in range(4)]\n",
    "    #dyna_model = TimeModel(actions_list)\n",
    "    dyna_model = simpleModel()\n",
    "    agent = DynaQPlus(actions_list,\n",
    "        eps=0.01,\n",
    "        n=10,\n",
    "        gamma=0.95,\n",
    "        alpha=0.1,\n",
    "        alpha_decay=0.001,\n",
    "        model_db=mdb,\n",
    "        score_db=sdb,\n",
    "        his_db=hdb,)\n",
    "    #agent = DynaQ(ncol, nrow, epsilon, alpha, gamma, n_planning)\n",
    "    num_episodes = 300  # 智能体在环境中运行多少条序列\n",
    "\n",
    "    return_list = []  # 记录每一条序列的回报\n",
    "    for i in range(10):  # 显示10个进度条\n",
    "        # tqdm的进度条功能\n",
    "        with tqdm(total=int(num_episodes / 10),\n",
    "                  desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n",
    "                episode_return = 0\n",
    "                state = env.reset()\n",
    "\n",
    "                done = False\n",
    "                while not done:\n",
    "                    model_id = f\"{n_planning}:model\"\n",
    "                    action = agent.act(state,model_id,request_id=1,save_his=False)\n",
    "                    #print(agent.model)\n",
    "                    #print(actions_list)\n",
    "                    #action = random.choice(actions_list)\n",
    "                    action =int( action[0])\n",
    "                    next_state, reward, done = env.step(action)\n",
    "\n",
    "                    agent.update_state_action_hist(1, state, action, next_state, reward, model_id)\n",
    "                    \n",
    "                    #print(next_state,reward,type(reward))\n",
    "                    episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n",
    "                    agent.learn(1,reward, model_id ,n_planning, dyna_model=dyna_model)\n",
    "                    #agent.update(state, action, reward, next_state,\"local_model\")\n",
    "\n",
    "                    #print(f,score_key1)\n",
    "                    state = next_state\n",
    "                return_list.append(episode_return)\n",
    "                if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n",
    "                    pbar.set_postfix({\n",
    "                        'episode':\n",
    "                        '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return':\n",
    "                        '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c264c3-5f1b-44a1-91f6-ac8b3e893775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-planning步数为：0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|█| 30/30 [00:03<00:00,  7.83it/s, episode=30, return=-341.400]\n",
      "Iteration 1: 100%|█| 30/30 [00:06<00:00,  4.78it/s, episode=60, return=-326.000]\n",
      "Iteration 2: 100%|█| 30/30 [00:06<00:00,  4.66it/s, episode=90, return=-268.700]\n",
      "Iteration 3: 100%|█| 30/30 [00:07<00:00,  3.87it/s, episode=120, return=-319.400\n",
      "Iteration 4: 100%|█| 30/30 [00:09<00:00,  3.24it/s, episode=150, return=-566.600\n",
      "Iteration 5: 100%|█| 30/30 [00:09<00:00,  3.09it/s, episode=180, return=-135.200\n",
      "Iteration 6: 100%|█| 30/30 [00:23<00:00,  1.29it/s, episode=210, return=-552.200\n",
      "Iteration 7: 100%|█| 30/30 [00:03<00:00,  8.58it/s, episode=240, return=-174.100\n",
      "Iteration 8: 100%|█| 30/30 [00:01<00:00, 18.57it/s, episode=270, return=-36.300]\n",
      "Iteration 9: 100%|█| 30/30 [00:00<00:00, 33.26it/s, episode=300, return=-22.900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-planning步数为：2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|█| 30/30 [00:49<00:00,  1.64s/it, episode=30, return=-1943.100\n",
      "Iteration 1: 100%|█| 30/30 [00:49<00:00,  1.64s/it, episode=60, return=-897.800]\n",
      "Iteration 2: 100%|█| 30/30 [01:03<00:00,  2.10s/it, episode=90, return=-1076.300\n",
      "Iteration 3: 100%|█| 30/30 [00:44<00:00,  1.49s/it, episode=120, return=-947.600\n",
      "Iteration 4:  67%|▋| 20/30 [00:55<00:28,  2.86s/it, episode=140, return=-1555.40"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "n_planning_list = [0,2,20]\n",
    "for n_planning in n_planning_list:\n",
    "    print('Q-planning步数为：%d' % n_planning)\n",
    "    time.sleep(0.5)\n",
    "    return_list = DynaQ_CliffWalking(n_planning)\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    plt.plot(episodes_list,\n",
    "             return_list,\n",
    "             label=str(n_planning) + ' planning steps')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Dyna-Q on {}'.format('Cliff Walking'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae8868a-7d1d-4140-8fe3-c3800ad6e1d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dyna_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdyna_model\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dyna_model' is not defined"
     ]
    }
   ],
   "source": [
    "dyna_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e76b39-8446-466a-b40b-39084359544b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
